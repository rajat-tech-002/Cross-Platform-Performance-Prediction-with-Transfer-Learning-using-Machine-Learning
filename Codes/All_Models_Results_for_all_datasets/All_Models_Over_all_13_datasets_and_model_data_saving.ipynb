{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_features(encode_decode, data_frame, encoder_isa=None, encoder_mem_type=None):\n",
    "    # Implement Categorical OneHot encoding for ISA and mem-type\n",
    "    if encode_decode == 'encode':\n",
    "        encoder_isa = ce.one_hot.OneHotEncoder(cols=['isa'])\n",
    "        encoder_mem_type = ce.one_hot.OneHotEncoder(cols=['mem-type'])\n",
    "        encoder_isa.fit(data_frame, verbose=1)\n",
    "        df_new1 = encoder_isa.transform(data_frame)\n",
    "        encoder_mem_type.fit(df_new1, verbose=1)\n",
    "        df_new = encoder_mem_type.transform(df_new1)\n",
    "        encoded_data_frame = df_new\n",
    "    else:\n",
    "        df_new1 = encoder_isa.transform(data_frame)\n",
    "        df_new = encoder_mem_type.transform(df_new1)\n",
    "        encoded_data_frame = df_new\n",
    "        \n",
    "    return encoded_data_frame, encoder_isa, encoder_mem_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_percentage_error(Y_test, Y_pred):\n",
    "    error = 0\n",
    "    for i in range(len(Y_test)):\n",
    "        if(Y_test[i]!= 0 ):\n",
    "            error = error + (abs(Y_test[i] - Y_pred[i]))/Y_test[i]\n",
    "        \n",
    "    error = error/ len(Y_test)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1 :dijkstra_physical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df, \n",
    "                                                                             encoder_isa = None, encoder_mem_type=None)\n",
    "    # total_data = encoded_data_frame.drop(columns = ['arch', 'arch1'])\n",
    "    \n",
    "    total_data = encoded_data_frame.drop(columns = ['arch'])\n",
    "    total_data = total_data.fillna(0)\n",
    "    X_columns = total_data.drop(columns = 'runtime').columns\n",
    "    X = total_data.drop(columns = ['runtime']).to_numpy()\n",
    "    Y = total_data['runtime'].to_numpy()\n",
    "    # X_columns = total_data.drop(columns = 'PS').columns\n",
    "    # X = total_data.drop(columns = ['runtime','PS']).to_numpy()\n",
    "    # Y = total_data['runtime'].to_numpy()\n",
    "    print('Data X and Y shape', X.shape, Y.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print('Train Test Split:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    # 1. SVR \n",
    "    best_svr = SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    \n",
    "    # 2. LR\n",
    "    best_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "    \n",
    "    # 3. RR\n",
    "    best_rr = linear_model.Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,\n",
    "      random_state=None, solver='lsqr', tol=0.001)\n",
    "    \n",
    "    # 4. KNN\n",
    "    best_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=15, p=1,\n",
    "                    weights='distance')\n",
    "    \n",
    "    # 5. GPR\n",
    "    best_gpr = GaussianProcessRegressor(alpha=0.01, copy_X_train=True, kernel=None,\n",
    "                         n_restarts_optimizer=0, normalize_y=True,\n",
    "                         optimizer='fmin_l_bfgs_b', random_state=None)\n",
    "    # 6. Decision Tree\n",
    "    best_dt = DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
    "                      max_features='sqrt', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=2, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, presort=False,\n",
    "                      random_state=None, splitter='best')\n",
    "    \n",
    "    # 7. Random Forest \n",
    "    best_rf = RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=3,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                      n_jobs=None, oob_score=False, random_state=None,\n",
    "                      verbose=0, warm_start='True')\n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    best_etr = ExtraTreesRegressor(bootstrap=False, criterion='friedman_mse', max_depth=3,\n",
    "                    max_features='auto', max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                    min_samples_leaf=1, min_samples_split=2,\n",
    "                    min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "                    oob_score=False, random_state=42, verbose=0,\n",
    "                    warm_start='True')\n",
    "    # 9. GBR\n",
    "    best_gbr = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='mae', init=None,\n",
    "                          learning_rate=0.1, loss='lad', max_depth=3,\n",
    "                          max_features=None, max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=1, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=42, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "    \n",
    "    # 10. XGB\n",
    "    best_xgb = xgb.XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.5, max_delta_step=0,\n",
    "             max_depth=4, min_child_weight=1, missing=None, n_estimators=10,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, validate_parameters=False, verbosity=1)\n",
    "    \n",
    "    best_models = [best_svr, best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr, best_gbr, best_xgb]\n",
    "    best_models_name = ['best_svr', 'best_lr', 'best_rr', 'best_knn', 'best_gpr', 'best_dt', 'best_rf', 'best_etr'\n",
    "                        , 'best_gbr', 'best_xgb']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mse', 'mape', 'mae' ])\n",
    "    \n",
    "    for model in best_models:\n",
    "        \n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mse_scores = []\n",
    "        mape_scores = []\n",
    "        mae_scores = []\n",
    "\n",
    "        # cv = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "        cv = ShuffleSplit(n_splits=10, random_state=0)\n",
    "        # print(cv)\n",
    "        \n",
    "        fold = 1\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            model_orig = model\n",
    "            # print(\"Train Index: \", train_index, \"\\n\")\n",
    "            # print(\"Test Index: \", test_index)\n",
    "\n",
    "            X_train_fold, X_test_fold, Y_train_fold, Y_test_fold = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "            # print(X_train_fold.shape, X_test_fold.shape, Y_train_fold.shape, Y_test_fold.shape)\n",
    "            model_orig.fit(X_train_fold, Y_train_fold)\n",
    "            Y_pred_fold = model_orig.predict(X_test_fold)\n",
    "            \n",
    "            # save the folds to disk\n",
    "            data = [X_train_fold, X_test_fold, Y_train_fold, Y_test_fold]\n",
    "            filename = path_for_saving_data + '/folds_data/' + best_models_name[k] +'_'+ str(fold) + '.pickle'\n",
    "            pickle.dump(data, open(filename, 'wb'))\n",
    "            \n",
    "            \n",
    "            # save the model to disk\n",
    "            filename = path_for_saving_data + '/models_data/' + best_models_name[k] + '_' + str(fold) + '.sav'\n",
    "            fold = fold + 1\n",
    "            pickle.dump(model_orig, open(filename, 'wb'))\n",
    "\n",
    "            # some time later...\n",
    "            '''\n",
    "            # load the model from disk\n",
    "            loaded_model = pickle.load(open(filename, 'rb'))\n",
    "            result = loaded_model.score(X_test, Y_test)\n",
    "            print(result)\n",
    "            '''\n",
    "            # scores.append(best_svr.score(X_test, y_test))\n",
    "            '''\n",
    "            plt.figure()\n",
    "            plt.plot(Y_test_fold, 'b')\n",
    "            plt.plot(Y_pred_fold, 'r')\n",
    "            '''\n",
    "            # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "            r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "            mse_scores.append(mean_squared_error(Y_test_fold, Y_pred_fold))\n",
    "            mape_scores.append(absolute_percentage_error(Y_test_fold, Y_pred_fold))\n",
    "            mae_scores.append(mean_absolute_error(Y_test_fold, Y_pred_fold))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores, 'mse': mse_scores, 'mape': mape_scores, 'mae': mae_scores }, ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv(path_for_saving_data + '.csv')\n",
    "        # print('MSE for 10 folds\\n', mse_scores)\n",
    "        # print('\\nR2 scores for 10 folds\\n', r2_scores)\n",
    "        # print('\\nMAPE for 10 folds\\n', mape_scores)\n",
    "        # print('\\nMAE scores for 10 folds\\n', mae_scores)\n",
    "        # print('\\nMean MSE = ', np.mean(mse_scores), '\\nMedian MSE = ', np.median(mse_scores))\n",
    "        # print('\\nMean R2 score =',np.mean(r2_scores), '\\nMedian R2 scores = ', np.median(r2_scores))\n",
    "        # print('\\nMean Absolute Percentage Error =',np.mean(mape_scores), \n",
    "        #       '\\nMedian Absolute Percentage Error =', np.median(mape_scores))    \n",
    "        # print('\\nMean MAE =',np.mean(mae_scores), \n",
    "        #      '\\nMedian MAE =', np.median(mae_scores)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data X and Y shape (52, 20) (52,)\n",
      "Train Test Split: (41, 20) (11, 20) (41,) (11,)\n",
      "Running model number: 1 with Model Name:  best_svr\n",
      "Running model number: 2 with Model Name:  best_lr\n",
      "Running model number: 3 with Model Name:  best_rr\n",
      "Running model number: 4 with Model Name:  best_knn\n",
      "Running model number: 5 with Model Name:  best_gpr\n",
      "Running model number: 6 with Model Name:  best_dt\n",
      "Running model number: 7 with Model Name:  best_rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model number: 8 with Model Name:  best_etr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model number: 9 with Model Name:  best_gbr\n",
      "Running model number: 10 with Model Name:  best_xgb\n",
      "[13:34:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:34:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:34:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:34:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:34:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:34:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:34:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:34:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:34:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[13:34:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  model_name       dataset_name  \\\n",
      "0   best_svr  dijkstra_physical   \n",
      "1    best_lr  dijkstra_physical   \n",
      "2    best_rr  dijkstra_physical   \n",
      "3   best_knn  dijkstra_physical   \n",
      "4   best_gpr  dijkstra_physical   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.706312387430097, 0.11420960680031889, 0.413...   \n",
      "1  [0.7358363002601873, 0.2558236703072164, 0.263...   \n",
      "2  [0.746938061499899, 0.26838277330657956, 0.539...   \n",
      "3  [0.771401290306746, 0.26355119334827026, 0.391...   \n",
      "4  [0.3832584181529476, 0.5257698801961834, 0.382...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [6867490.713539857, 66689948.15289214, 1126236...   \n",
      "1  [6177113.630850524, 56028018.8460269, 14148130...   \n",
      "2  [5917513.84197788, 55082461.14489979, 8836948....   \n",
      "3  [5345474.064119581, 55446224.19696031, 1168005...   \n",
      "4  [14421674.271264665, 35704137.621107526, 11854...   \n",
      "\n",
      "                                                mape  \\\n",
      "0  [0.08944596084129125, 0.15483933980283732, 0.0...   \n",
      "1  [0.05663991579667571, 0.11017101774459677, 0.1...   \n",
      "2  [0.05797051247533702, 0.11783776190893851, 0.0...   \n",
      "3  [0.0625447669398206, 0.11457417058993095, 0.09...   \n",
      "4  [0.09964799362540648, 0.11296509645681578, 0.1...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [1932.5063803989117, 5451.625134251278, 2655.3...  \n",
      "1  [1581.9824509640791, 4291.516023534958, 2999.4...  \n",
      "2  [1595.7386637591753, 4545.601820986733, 2302.9...  \n",
      "3  [1803.1520961928952, 4462.539297732807, 2766.3...  \n",
      "4  [2920.7710260262143, 4092.805861227611, 3078.4...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'dijkstra_physical'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\\\Performance\\\\PhysicalSystems\\\\dijkstra_physical.csv'\n",
    "path_for_saving_data = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\results_with_normalization_without_PCA\\\\' + dataset_name\n",
    "process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2 : dijkstra_simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df, \n",
    "                                                                             encoder_isa = None, encoder_mem_type=None)\n",
    "    # total_data = encoded_data_frame.drop(columns = ['arch', 'arch1'])\n",
    "    \n",
    "    total_data = encoded_data_frame.drop(columns = ['arch'])\n",
    "    total_data = total_data.fillna(0)\n",
    "    X_columns = total_data.drop(columns = 'runtime').columns\n",
    "    X = total_data.drop(columns = ['runtime']).to_numpy()\n",
    "    Y = total_data['runtime'].to_numpy()\n",
    "    # X_columns = total_data.drop(columns = 'PS').columns\n",
    "    # X = total_data.drop(columns = ['runtime','PS']).to_numpy()\n",
    "    # Y = total_data['runtime'].to_numpy()\n",
    "    print('Data X and Y shape', X.shape, Y.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print('Train Test Split:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    # 1. SVR \n",
    "    best_svr = SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    \n",
    "    # 2. LR\n",
    "    best_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "    \n",
    "    # 3. RR\n",
    "    best_rr = linear_model.Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,\n",
    "      random_state=None, solver='lsqr', tol=0.001)\n",
    "    \n",
    "    # 4. KNN\n",
    "    best_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=15, p=1,\n",
    "                    weights='distance')\n",
    "    \n",
    "    # 5. GPR\n",
    "    best_gpr = GaussianProcessRegressor(alpha=0.01, copy_X_train=True, kernel=None,\n",
    "                         n_restarts_optimizer=0, normalize_y=True,\n",
    "                         optimizer='fmin_l_bfgs_b', random_state=None)\n",
    "    # 6. Decision Tree\n",
    "    best_dt = DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
    "                      max_features='sqrt', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=2, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, presort=False,\n",
    "                      random_state=None, splitter='best')\n",
    "    \n",
    "    # 7. Random Forest \n",
    "    best_rf = RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=3,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                      n_jobs=None, oob_score=False, random_state=None,\n",
    "                      verbose=0, warm_start='True')\n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    best_etr = ExtraTreesRegressor(bootstrap=False, criterion='friedman_mse', max_depth=3,\n",
    "                    max_features='auto', max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                    min_samples_leaf=1, min_samples_split=2,\n",
    "                    min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "                    oob_score=False, random_state=42, verbose=0,\n",
    "                    warm_start='True')\n",
    "    # 9. GBR\n",
    "    best_gbr = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='mae', init=None,\n",
    "                          learning_rate=0.1, loss='lad', max_depth=3,\n",
    "                          max_features=None, max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=1, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=42, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "    \n",
    "    # 10. XGB\n",
    "    best_xgb = xgb.XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.5, max_delta_step=0,\n",
    "             max_depth=4, min_child_weight=1, missing=None, n_estimators=10,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, validate_parameters=False, verbosity=1)\n",
    "    \n",
    "    best_models = [best_svr, best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr, best_gbr, best_xgb]\n",
    "    best_models_name = ['best_svr', 'best_lr', 'best_rr', 'best_knn', 'best_gpr', 'best_dt', 'best_rf', 'best_etr'\n",
    "                        , 'best_gbr', 'best_xgb']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mse', 'mape', 'mae' ])\n",
    "    \n",
    "    for model in best_models:\n",
    "        \n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mse_scores = []\n",
    "        mape_scores = []\n",
    "        mae_scores = []\n",
    "\n",
    "        # cv = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "        cv = ShuffleSplit(n_splits=10, random_state=0)\n",
    "        # print(cv)\n",
    "        \n",
    "        fold = 1\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            model_orig = model\n",
    "            # print(\"Train Index: \", train_index, \"\\n\")\n",
    "            # print(\"Test Index: \", test_index)\n",
    "\n",
    "            X_train_fold, X_test_fold, Y_train_fold, Y_test_fold = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "            # print(X_train_fold.shape, X_test_fold.shape, Y_train_fold.shape, Y_test_fold.shape)\n",
    "            model_orig.fit(X_train_fold, Y_train_fold)\n",
    "            Y_pred_fold = model_orig.predict(X_test_fold)\n",
    "            \n",
    "            # save the folds to disk\n",
    "            data = [X_train_fold, X_test_fold, Y_train_fold, Y_test_fold]\n",
    "            filename = path_for_saving_data + '/folds_data/' + best_models_name[k] +'_'+ str(fold) + '.pickle'\n",
    "            pickle.dump(data, open(filename, 'wb'))\n",
    "            \n",
    "            \n",
    "            # save the model to disk\n",
    "            filename = path_for_saving_data + '/models_data/' + best_models_name[k] + '_' + str(fold) + '.sav'\n",
    "            fold = fold + 1\n",
    "            pickle.dump(model_orig, open(filename, 'wb'))\n",
    "\n",
    "            # some time later...\n",
    "            '''\n",
    "            # load the model from disk\n",
    "            loaded_model = pickle.load(open(filename, 'rb'))\n",
    "            result = loaded_model.score(X_test, Y_test)\n",
    "            print(result)\n",
    "            '''\n",
    "            # scores.append(best_svr.score(X_test, y_test))\n",
    "            '''\n",
    "            plt.figure()\n",
    "            plt.plot(Y_test_fold, 'b')\n",
    "            plt.plot(Y_pred_fold, 'r')\n",
    "            '''\n",
    "            # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "            r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "            mse_scores.append(mean_squared_error(Y_test_fold, Y_pred_fold))\n",
    "            mape_scores.append(absolute_percentage_error(Y_test_fold, Y_pred_fold))\n",
    "            mae_scores.append(mean_absolute_error(Y_test_fold, Y_pred_fold))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores, 'mse': mse_scores, 'mape': mape_scores, 'mae': mae_scores }, ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv(path_for_saving_data + '.csv')\n",
    "        # print('MSE for 10 folds\\n', mse_scores)\n",
    "        # print('\\nR2 scores for 10 folds\\n', r2_scores)\n",
    "        # print('\\nMAPE for 10 folds\\n', mape_scores)\n",
    "        # print('\\nMAE scores for 10 folds\\n', mae_scores)\n",
    "        # print('\\nMean MSE = ', np.mean(mse_scores), '\\nMedian MSE = ', np.median(mse_scores))\n",
    "        # print('\\nMean R2 score =',np.mean(r2_scores), '\\nMedian R2 scores = ', np.median(r2_scores))\n",
    "        # print('\\nMean Absolute Percentage Error =',np.mean(mape_scores), \n",
    "        #       '\\nMedian Absolute Percentage Error =', np.median(mape_scores))    \n",
    "        # print('\\nMean MAE =',np.mean(mae_scores), \n",
    "        #      '\\nMedian MAE =', np.median(mae_scores)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'dijkstra_physical'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\\\Performance\\\\PhysicalSystems\\\\dijkstra_physical.csv'\n",
    "path_for_saving_data = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\results_with_normalization_without_PCA\\\\' + dataset_name\n",
    "process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 3 : qsort_physical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df, \n",
    "                                                                             encoder_isa = None, encoder_mem_type=None)\n",
    "    # total_data = encoded_data_frame.drop(columns = ['arch', 'arch1'])\n",
    "    \n",
    "    total_data = encoded_data_frame.drop(columns = ['arch'])\n",
    "    total_data = total_data.fillna(0)\n",
    "    X_columns = total_data.drop(columns = 'runtime').columns\n",
    "    X = total_data.drop(columns = ['runtime']).to_numpy()\n",
    "    Y = total_data['runtime'].to_numpy()\n",
    "    # X_columns = total_data.drop(columns = 'PS').columns\n",
    "    # X = total_data.drop(columns = ['runtime','PS']).to_numpy()\n",
    "    # Y = total_data['runtime'].to_numpy()\n",
    "    print('Data X and Y shape', X.shape, Y.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print('Train Test Split:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    # 1. SVR \n",
    "    best_svr = SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    \n",
    "    # 2. LR\n",
    "    best_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "    \n",
    "    # 3. RR\n",
    "    best_rr = linear_model.Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,\n",
    "      random_state=None, solver='lsqr', tol=0.001)\n",
    "    \n",
    "    # 4. KNN\n",
    "    best_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=15, p=1,\n",
    "                    weights='distance')\n",
    "    \n",
    "    # 5. GPR\n",
    "    best_gpr = GaussianProcessRegressor(alpha=0.01, copy_X_train=True, kernel=None,\n",
    "                         n_restarts_optimizer=0, normalize_y=True,\n",
    "                         optimizer='fmin_l_bfgs_b', random_state=None)\n",
    "    # 6. Decision Tree\n",
    "    best_dt = DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
    "                      max_features='sqrt', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=2, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, presort=False,\n",
    "                      random_state=None, splitter='best')\n",
    "    \n",
    "    # 7. Random Forest \n",
    "    best_rf = RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=3,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                      n_jobs=None, oob_score=False, random_state=None,\n",
    "                      verbose=0, warm_start='True')\n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    best_etr = ExtraTreesRegressor(bootstrap=False, criterion='friedman_mse', max_depth=3,\n",
    "                    max_features='auto', max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                    min_samples_leaf=1, min_samples_split=2,\n",
    "                    min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "                    oob_score=False, random_state=42, verbose=0,\n",
    "                    warm_start='True')\n",
    "    # 9. GBR\n",
    "    best_gbr = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='mae', init=None,\n",
    "                          learning_rate=0.1, loss='lad', max_depth=3,\n",
    "                          max_features=None, max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=1, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=42, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "    \n",
    "    # 10. XGB\n",
    "    best_xgb = xgb.XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.5, max_delta_step=0,\n",
    "             max_depth=4, min_child_weight=1, missing=None, n_estimators=10,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, validate_parameters=False, verbosity=1)\n",
    "    \n",
    "    best_models = [best_svr, best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr, best_gbr, best_xgb]\n",
    "    best_models_name = ['best_svr', 'best_lr', 'best_rr', 'best_knn', 'best_gpr', 'best_dt', 'best_rf', 'best_etr'\n",
    "                        , 'best_gbr', 'best_xgb']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mse', 'mape', 'mae' ])\n",
    "    \n",
    "    for model in best_models:\n",
    "        \n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mse_scores = []\n",
    "        mape_scores = []\n",
    "        mae_scores = []\n",
    "\n",
    "        # cv = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "        cv = ShuffleSplit(n_splits=10, random_state=0)\n",
    "        # print(cv)\n",
    "        \n",
    "        fold = 1\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            model_orig = model\n",
    "            # print(\"Train Index: \", train_index, \"\\n\")\n",
    "            # print(\"Test Index: \", test_index)\n",
    "\n",
    "            X_train_fold, X_test_fold, Y_train_fold, Y_test_fold = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "            # print(X_train_fold.shape, X_test_fold.shape, Y_train_fold.shape, Y_test_fold.shape)\n",
    "            model_orig.fit(X_train_fold, Y_train_fold)\n",
    "            Y_pred_fold = model_orig.predict(X_test_fold)\n",
    "            \n",
    "            # save the folds to disk\n",
    "            data = [X_train_fold, X_test_fold, Y_train_fold, Y_test_fold]\n",
    "            filename = path_for_saving_data + '/folds_data/' + best_models_name[k] +'_'+ str(fold) + '.pickle'\n",
    "            pickle.dump(data, open(filename, 'wb'))\n",
    "            \n",
    "            \n",
    "            # save the model to disk\n",
    "            filename = path_for_saving_data + '/models_data/' + best_models_name[k] + '_' + str(fold) + '.sav'\n",
    "            fold = fold + 1\n",
    "            pickle.dump(model_orig, open(filename, 'wb'))\n",
    "\n",
    "            # some time later...\n",
    "            '''\n",
    "            # load the model from disk\n",
    "            loaded_model = pickle.load(open(filename, 'rb'))\n",
    "            result = loaded_model.score(X_test, Y_test)\n",
    "            print(result)\n",
    "            '''\n",
    "            # scores.append(best_svr.score(X_test, y_test))\n",
    "            '''\n",
    "            plt.figure()\n",
    "            plt.plot(Y_test_fold, 'b')\n",
    "            plt.plot(Y_pred_fold, 'r')\n",
    "            '''\n",
    "            # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "            r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "            mse_scores.append(mean_squared_error(Y_test_fold, Y_pred_fold))\n",
    "            mape_scores.append(absolute_percentage_error(Y_test_fold, Y_pred_fold))\n",
    "            mae_scores.append(mean_absolute_error(Y_test_fold, Y_pred_fold))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores, 'mse': mse_scores, 'mape': mape_scores, 'mae': mae_scores }, ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv(path_for_saving_data + '.csv')\n",
    "        # print('MSE for 10 folds\\n', mse_scores)\n",
    "        # print('\\nR2 scores for 10 folds\\n', r2_scores)\n",
    "        # print('\\nMAPE for 10 folds\\n', mape_scores)\n",
    "        # print('\\nMAE scores for 10 folds\\n', mae_scores)\n",
    "        # print('\\nMean MSE = ', np.mean(mse_scores), '\\nMedian MSE = ', np.median(mse_scores))\n",
    "        # print('\\nMean R2 score =',np.mean(r2_scores), '\\nMedian R2 scores = ', np.median(r2_scores))\n",
    "        # print('\\nMean Absolute Percentage Error =',np.mean(mape_scores), \n",
    "        #       '\\nMedian Absolute Percentage Error =', np.median(mape_scores))    \n",
    "        # print('\\nMean MAE =',np.mean(mae_scores), \n",
    "        #      '\\nMedian MAE =', np.median(mae_scores)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'dijkstra_physical'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\\\Performance\\\\PhysicalSystems\\\\dijkstra_physical.csv'\n",
    "path_for_saving_data = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\results_with_normalization_without_PCA\\\\' + dataset_name\n",
    "process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 4 : qsort_simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df, \n",
    "                                                                             encoder_isa = None, encoder_mem_type=None)\n",
    "    # total_data = encoded_data_frame.drop(columns = ['arch', 'arch1'])\n",
    "    \n",
    "    total_data = encoded_data_frame.drop(columns = ['arch'])\n",
    "    total_data = total_data.fillna(0)\n",
    "    X_columns = total_data.drop(columns = 'runtime').columns\n",
    "    X = total_data.drop(columns = ['runtime']).to_numpy()\n",
    "    Y = total_data['runtime'].to_numpy()\n",
    "    # X_columns = total_data.drop(columns = 'PS').columns\n",
    "    # X = total_data.drop(columns = ['runtime','PS']).to_numpy()\n",
    "    # Y = total_data['runtime'].to_numpy()\n",
    "    print('Data X and Y shape', X.shape, Y.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print('Train Test Split:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    # 1. SVR \n",
    "    best_svr = SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    \n",
    "    # 2. LR\n",
    "    best_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "    \n",
    "    # 3. RR\n",
    "    best_rr = linear_model.Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,\n",
    "      random_state=None, solver='lsqr', tol=0.001)\n",
    "    \n",
    "    # 4. KNN\n",
    "    best_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=15, p=1,\n",
    "                    weights='distance')\n",
    "    \n",
    "    # 5. GPR\n",
    "    best_gpr = GaussianProcessRegressor(alpha=0.01, copy_X_train=True, kernel=None,\n",
    "                         n_restarts_optimizer=0, normalize_y=True,\n",
    "                         optimizer='fmin_l_bfgs_b', random_state=None)\n",
    "    # 6. Decision Tree\n",
    "    best_dt = DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
    "                      max_features='sqrt', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=2, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, presort=False,\n",
    "                      random_state=None, splitter='best')\n",
    "    \n",
    "    # 7. Random Forest \n",
    "    best_rf = RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=3,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                      n_jobs=None, oob_score=False, random_state=None,\n",
    "                      verbose=0, warm_start='True')\n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    best_etr = ExtraTreesRegressor(bootstrap=False, criterion='friedman_mse', max_depth=3,\n",
    "                    max_features='auto', max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                    min_samples_leaf=1, min_samples_split=2,\n",
    "                    min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "                    oob_score=False, random_state=42, verbose=0,\n",
    "                    warm_start='True')\n",
    "    # 9. GBR\n",
    "    best_gbr = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='mae', init=None,\n",
    "                          learning_rate=0.1, loss='lad', max_depth=3,\n",
    "                          max_features=None, max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=1, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=42, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "    \n",
    "    # 10. XGB\n",
    "    best_xgb = xgb.XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.5, max_delta_step=0,\n",
    "             max_depth=4, min_child_weight=1, missing=None, n_estimators=10,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, validate_parameters=False, verbosity=1)\n",
    "    \n",
    "    best_models = [best_svr, best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr, best_gbr, best_xgb]\n",
    "    best_models_name = ['best_svr', 'best_lr', 'best_rr', 'best_knn', 'best_gpr', 'best_dt', 'best_rf', 'best_etr'\n",
    "                        , 'best_gbr', 'best_xgb']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mse', 'mape', 'mae' ])\n",
    "    \n",
    "    for model in best_models:\n",
    "        \n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mse_scores = []\n",
    "        mape_scores = []\n",
    "        mae_scores = []\n",
    "\n",
    "        # cv = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "        cv = ShuffleSplit(n_splits=10, random_state=0)\n",
    "        # print(cv)\n",
    "        \n",
    "        fold = 1\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            model_orig = model\n",
    "            # print(\"Train Index: \", train_index, \"\\n\")\n",
    "            # print(\"Test Index: \", test_index)\n",
    "\n",
    "            X_train_fold, X_test_fold, Y_train_fold, Y_test_fold = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "            # print(X_train_fold.shape, X_test_fold.shape, Y_train_fold.shape, Y_test_fold.shape)\n",
    "            model_orig.fit(X_train_fold, Y_train_fold)\n",
    "            Y_pred_fold = model_orig.predict(X_test_fold)\n",
    "            \n",
    "            # save the folds to disk\n",
    "            data = [X_train_fold, X_test_fold, Y_train_fold, Y_test_fold]\n",
    "            filename = path_for_saving_data + '/folds_data/' + best_models_name[k] +'_'+ str(fold) + '.pickle'\n",
    "            pickle.dump(data, open(filename, 'wb'))\n",
    "            \n",
    "            \n",
    "            # save the model to disk\n",
    "            filename = path_for_saving_data + '/models_data/' + best_models_name[k] + '_' + str(fold) + '.sav'\n",
    "            fold = fold + 1\n",
    "            pickle.dump(model_orig, open(filename, 'wb'))\n",
    "\n",
    "            # some time later...\n",
    "            '''\n",
    "            # load the model from disk\n",
    "            loaded_model = pickle.load(open(filename, 'rb'))\n",
    "            result = loaded_model.score(X_test, Y_test)\n",
    "            print(result)\n",
    "            '''\n",
    "            # scores.append(best_svr.score(X_test, y_test))\n",
    "            '''\n",
    "            plt.figure()\n",
    "            plt.plot(Y_test_fold, 'b')\n",
    "            plt.plot(Y_pred_fold, 'r')\n",
    "            '''\n",
    "            # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "            r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "            mse_scores.append(mean_squared_error(Y_test_fold, Y_pred_fold))\n",
    "            mape_scores.append(absolute_percentage_error(Y_test_fold, Y_pred_fold))\n",
    "            mae_scores.append(mean_absolute_error(Y_test_fold, Y_pred_fold))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores, 'mse': mse_scores, 'mape': mape_scores, 'mae': mae_scores }, ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv(path_for_saving_data + '.csv')\n",
    "        # print('MSE for 10 folds\\n', mse_scores)\n",
    "        # print('\\nR2 scores for 10 folds\\n', r2_scores)\n",
    "        # print('\\nMAPE for 10 folds\\n', mape_scores)\n",
    "        # print('\\nMAE scores for 10 folds\\n', mae_scores)\n",
    "        # print('\\nMean MSE = ', np.mean(mse_scores), '\\nMedian MSE = ', np.median(mse_scores))\n",
    "        # print('\\nMean R2 score =',np.mean(r2_scores), '\\nMedian R2 scores = ', np.median(r2_scores))\n",
    "        # print('\\nMean Absolute Percentage Error =',np.mean(mape_scores), \n",
    "        #       '\\nMedian Absolute Percentage Error =', np.median(mape_scores))    \n",
    "        # print('\\nMean MAE =',np.mean(mae_scores), \n",
    "        #      '\\nMedian MAE =', np.median(mae_scores)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'dijkstra_physical'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\\\Performance\\\\PhysicalSystems\\\\dijkstra_physical.csv'\n",
    "path_for_saving_data = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\results_with_normalization_without_PCA\\\\' + dataset_name\n",
    "process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 5 : mantevominiFE_physical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df, \n",
    "                                                                             encoder_isa = None, encoder_mem_type=None)\n",
    "    # total_data = encoded_data_frame.drop(columns = ['arch', 'arch1'])\n",
    "    \n",
    "    total_data = encoded_data_frame.drop(columns = ['arch'])\n",
    "    total_data = total_data.fillna(0)\n",
    "    X_columns = total_data.drop(columns = 'runtime').columns\n",
    "    X = total_data.drop(columns = ['runtime']).to_numpy()\n",
    "    Y = total_data['runtime'].to_numpy()\n",
    "    # X_columns = total_data.drop(columns = 'PS').columns\n",
    "    # X = total_data.drop(columns = ['runtime','PS']).to_numpy()\n",
    "    # Y = total_data['runtime'].to_numpy()\n",
    "    print('Data X and Y shape', X.shape, Y.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print('Train Test Split:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    # 1. SVR \n",
    "    best_svr = SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    \n",
    "    # 2. LR\n",
    "    best_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "    \n",
    "    # 3. RR\n",
    "    best_rr = linear_model.Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,\n",
    "      random_state=None, solver='lsqr', tol=0.001)\n",
    "    \n",
    "    # 4. KNN\n",
    "    best_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=15, p=1,\n",
    "                    weights='distance')\n",
    "    \n",
    "    # 5. GPR\n",
    "    best_gpr = GaussianProcessRegressor(alpha=0.01, copy_X_train=True, kernel=None,\n",
    "                         n_restarts_optimizer=0, normalize_y=True,\n",
    "                         optimizer='fmin_l_bfgs_b', random_state=None)\n",
    "    # 6. Decision Tree\n",
    "    best_dt = DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
    "                      max_features='sqrt', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=2, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, presort=False,\n",
    "                      random_state=None, splitter='best')\n",
    "    \n",
    "    # 7. Random Forest \n",
    "    best_rf = RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=3,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                      n_jobs=None, oob_score=False, random_state=None,\n",
    "                      verbose=0, warm_start='True')\n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    best_etr = ExtraTreesRegressor(bootstrap=False, criterion='friedman_mse', max_depth=3,\n",
    "                    max_features='auto', max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                    min_samples_leaf=1, min_samples_split=2,\n",
    "                    min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "                    oob_score=False, random_state=42, verbose=0,\n",
    "                    warm_start='True')\n",
    "    # 9. GBR\n",
    "    best_gbr = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='mae', init=None,\n",
    "                          learning_rate=0.1, loss='lad', max_depth=3,\n",
    "                          max_features=None, max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=1, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=42, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "    \n",
    "    # 10. XGB\n",
    "    best_xgb = xgb.XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.5, max_delta_step=0,\n",
    "             max_depth=4, min_child_weight=1, missing=None, n_estimators=10,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, validate_parameters=False, verbosity=1)\n",
    "    \n",
    "    best_models = [best_svr, best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr, best_gbr, best_xgb]\n",
    "    best_models_name = ['best_svr', 'best_lr', 'best_rr', 'best_knn', 'best_gpr', 'best_dt', 'best_rf', 'best_etr'\n",
    "                        , 'best_gbr', 'best_xgb']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mse', 'mape', 'mae' ])\n",
    "    \n",
    "    for model in best_models:\n",
    "        \n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mse_scores = []\n",
    "        mape_scores = []\n",
    "        mae_scores = []\n",
    "\n",
    "        # cv = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "        cv = ShuffleSplit(n_splits=10, random_state=0)\n",
    "        # print(cv)\n",
    "        \n",
    "        fold = 1\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            model_orig = model\n",
    "            # print(\"Train Index: \", train_index, \"\\n\")\n",
    "            # print(\"Test Index: \", test_index)\n",
    "\n",
    "            X_train_fold, X_test_fold, Y_train_fold, Y_test_fold = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "            # print(X_train_fold.shape, X_test_fold.shape, Y_train_fold.shape, Y_test_fold.shape)\n",
    "            model_orig.fit(X_train_fold, Y_train_fold)\n",
    "            Y_pred_fold = model_orig.predict(X_test_fold)\n",
    "            \n",
    "            # save the folds to disk\n",
    "            data = [X_train_fold, X_test_fold, Y_train_fold, Y_test_fold]\n",
    "            filename = path_for_saving_data + '/folds_data/' + best_models_name[k] +'_'+ str(fold) + '.pickle'\n",
    "            pickle.dump(data, open(filename, 'wb'))\n",
    "            \n",
    "            \n",
    "            # save the model to disk\n",
    "            filename = path_for_saving_data + '/models_data/' + best_models_name[k] + '_' + str(fold) + '.sav'\n",
    "            fold = fold + 1\n",
    "            pickle.dump(model_orig, open(filename, 'wb'))\n",
    "\n",
    "            # some time later...\n",
    "            '''\n",
    "            # load the model from disk\n",
    "            loaded_model = pickle.load(open(filename, 'rb'))\n",
    "            result = loaded_model.score(X_test, Y_test)\n",
    "            print(result)\n",
    "            '''\n",
    "            # scores.append(best_svr.score(X_test, y_test))\n",
    "            '''\n",
    "            plt.figure()\n",
    "            plt.plot(Y_test_fold, 'b')\n",
    "            plt.plot(Y_pred_fold, 'r')\n",
    "            '''\n",
    "            # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "            r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "            mse_scores.append(mean_squared_error(Y_test_fold, Y_pred_fold))\n",
    "            mape_scores.append(absolute_percentage_error(Y_test_fold, Y_pred_fold))\n",
    "            mae_scores.append(mean_absolute_error(Y_test_fold, Y_pred_fold))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores, 'mse': mse_scores, 'mape': mape_scores, 'mae': mae_scores }, ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv(path_for_saving_data + '.csv')\n",
    "        # print('MSE for 10 folds\\n', mse_scores)\n",
    "        # print('\\nR2 scores for 10 folds\\n', r2_scores)\n",
    "        # print('\\nMAPE for 10 folds\\n', mape_scores)\n",
    "        # print('\\nMAE scores for 10 folds\\n', mae_scores)\n",
    "        # print('\\nMean MSE = ', np.mean(mse_scores), '\\nMedian MSE = ', np.median(mse_scores))\n",
    "        # print('\\nMean R2 score =',np.mean(r2_scores), '\\nMedian R2 scores = ', np.median(r2_scores))\n",
    "        # print('\\nMean Absolute Percentage Error =',np.mean(mape_scores), \n",
    "        #       '\\nMedian Absolute Percentage Error =', np.median(mape_scores))    \n",
    "        # print('\\nMean MAE =',np.mean(mae_scores), \n",
    "        #      '\\nMedian MAE =', np.median(mae_scores)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'dijkstra_physical'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\\\Performance\\\\PhysicalSystems\\\\dijkstra_physical.csv'\n",
    "path_for_saving_data = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\results_with_normalization_without_PCA\\\\' + dataset_name\n",
    "process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 6 : npbEP_physical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df, \n",
    "                                                                             encoder_isa = None, encoder_mem_type=None)\n",
    "    # total_data = encoded_data_frame.drop(columns = ['arch', 'arch1'])\n",
    "    \n",
    "    total_data = encoded_data_frame.drop(columns = ['arch'])\n",
    "    total_data = total_data.fillna(0)\n",
    "    X_columns = total_data.drop(columns = 'runtime').columns\n",
    "    X = total_data.drop(columns = ['runtime']).to_numpy()\n",
    "    Y = total_data['runtime'].to_numpy()\n",
    "    # X_columns = total_data.drop(columns = 'PS').columns\n",
    "    # X = total_data.drop(columns = ['runtime','PS']).to_numpy()\n",
    "    # Y = total_data['runtime'].to_numpy()\n",
    "    print('Data X and Y shape', X.shape, Y.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print('Train Test Split:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    # 1. SVR \n",
    "    best_svr = SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    \n",
    "    # 2. LR\n",
    "    best_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "    \n",
    "    # 3. RR\n",
    "    best_rr = linear_model.Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,\n",
    "      random_state=None, solver='lsqr', tol=0.001)\n",
    "    \n",
    "    # 4. KNN\n",
    "    best_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=15, p=1,\n",
    "                    weights='distance')\n",
    "    \n",
    "    # 5. GPR\n",
    "    best_gpr = GaussianProcessRegressor(alpha=0.01, copy_X_train=True, kernel=None,\n",
    "                         n_restarts_optimizer=0, normalize_y=True,\n",
    "                         optimizer='fmin_l_bfgs_b', random_state=None)\n",
    "    # 6. Decision Tree\n",
    "    best_dt = DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
    "                      max_features='sqrt', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=2, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, presort=False,\n",
    "                      random_state=None, splitter='best')\n",
    "    \n",
    "    # 7. Random Forest \n",
    "    best_rf = RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=3,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                      n_jobs=None, oob_score=False, random_state=None,\n",
    "                      verbose=0, warm_start='True')\n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    best_etr = ExtraTreesRegressor(bootstrap=False, criterion='friedman_mse', max_depth=3,\n",
    "                    max_features='auto', max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                    min_samples_leaf=1, min_samples_split=2,\n",
    "                    min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "                    oob_score=False, random_state=42, verbose=0,\n",
    "                    warm_start='True')\n",
    "    # 9. GBR\n",
    "    best_gbr = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='mae', init=None,\n",
    "                          learning_rate=0.1, loss='lad', max_depth=3,\n",
    "                          max_features=None, max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=1, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=42, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "    \n",
    "    # 10. XGB\n",
    "    best_xgb = xgb.XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.5, max_delta_step=0,\n",
    "             max_depth=4, min_child_weight=1, missing=None, n_estimators=10,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, validate_parameters=False, verbosity=1)\n",
    "    \n",
    "    best_models = [best_svr, best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr, best_gbr, best_xgb]\n",
    "    best_models_name = ['best_svr', 'best_lr', 'best_rr', 'best_knn', 'best_gpr', 'best_dt', 'best_rf', 'best_etr'\n",
    "                        , 'best_gbr', 'best_xgb']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mse', 'mape', 'mae' ])\n",
    "    \n",
    "    for model in best_models:\n",
    "        \n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mse_scores = []\n",
    "        mape_scores = []\n",
    "        mae_scores = []\n",
    "\n",
    "        # cv = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "        cv = ShuffleSplit(n_splits=10, random_state=0)\n",
    "        # print(cv)\n",
    "        \n",
    "        fold = 1\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            model_orig = model\n",
    "            # print(\"Train Index: \", train_index, \"\\n\")\n",
    "            # print(\"Test Index: \", test_index)\n",
    "\n",
    "            X_train_fold, X_test_fold, Y_train_fold, Y_test_fold = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "            # print(X_train_fold.shape, X_test_fold.shape, Y_train_fold.shape, Y_test_fold.shape)\n",
    "            model_orig.fit(X_train_fold, Y_train_fold)\n",
    "            Y_pred_fold = model_orig.predict(X_test_fold)\n",
    "            \n",
    "            # save the folds to disk\n",
    "            data = [X_train_fold, X_test_fold, Y_train_fold, Y_test_fold]\n",
    "            filename = path_for_saving_data + '/folds_data/' + best_models_name[k] +'_'+ str(fold) + '.pickle'\n",
    "            pickle.dump(data, open(filename, 'wb'))\n",
    "            \n",
    "            \n",
    "            # save the model to disk\n",
    "            filename = path_for_saving_data + '/models_data/' + best_models_name[k] + '_' + str(fold) + '.sav'\n",
    "            fold = fold + 1\n",
    "            pickle.dump(model_orig, open(filename, 'wb'))\n",
    "\n",
    "            # some time later...\n",
    "            '''\n",
    "            # load the model from disk\n",
    "            loaded_model = pickle.load(open(filename, 'rb'))\n",
    "            result = loaded_model.score(X_test, Y_test)\n",
    "            print(result)\n",
    "            '''\n",
    "            # scores.append(best_svr.score(X_test, y_test))\n",
    "            '''\n",
    "            plt.figure()\n",
    "            plt.plot(Y_test_fold, 'b')\n",
    "            plt.plot(Y_pred_fold, 'r')\n",
    "            '''\n",
    "            # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "            r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "            mse_scores.append(mean_squared_error(Y_test_fold, Y_pred_fold))\n",
    "            mape_scores.append(absolute_percentage_error(Y_test_fold, Y_pred_fold))\n",
    "            mae_scores.append(mean_absolute_error(Y_test_fold, Y_pred_fold))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores, 'mse': mse_scores, 'mape': mape_scores, 'mae': mae_scores }, ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv(path_for_saving_data + '.csv')\n",
    "        # print('MSE for 10 folds\\n', mse_scores)\n",
    "        # print('\\nR2 scores for 10 folds\\n', r2_scores)\n",
    "        # print('\\nMAPE for 10 folds\\n', mape_scores)\n",
    "        # print('\\nMAE scores for 10 folds\\n', mae_scores)\n",
    "        # print('\\nMean MSE = ', np.mean(mse_scores), '\\nMedian MSE = ', np.median(mse_scores))\n",
    "        # print('\\nMean R2 score =',np.mean(r2_scores), '\\nMedian R2 scores = ', np.median(r2_scores))\n",
    "        # print('\\nMean Absolute Percentage Error =',np.mean(mape_scores), \n",
    "        #       '\\nMedian Absolute Percentage Error =', np.median(mape_scores))    \n",
    "        # print('\\nMean MAE =',np.mean(mae_scores), \n",
    "        #      '\\nMedian MAE =', np.median(mae_scores)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'dijkstra_physical'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\\\Performance\\\\PhysicalSystems\\\\dijkstra_physical.csv'\n",
    "path_for_saving_data = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\results_with_normalization_without_PCA\\\\' + dataset_name\n",
    "process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 7 : npbMG_physical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df, \n",
    "                                                                             encoder_isa = None, encoder_mem_type=None)\n",
    "    # total_data = encoded_data_frame.drop(columns = ['arch', 'arch1'])\n",
    "    \n",
    "    total_data = encoded_data_frame.drop(columns = ['arch'])\n",
    "    total_data = total_data.fillna(0)\n",
    "    X_columns = total_data.drop(columns = 'runtime').columns\n",
    "    X = total_data.drop(columns = ['runtime']).to_numpy()\n",
    "    Y = total_data['runtime'].to_numpy()\n",
    "    # X_columns = total_data.drop(columns = 'PS').columns\n",
    "    # X = total_data.drop(columns = ['runtime','PS']).to_numpy()\n",
    "    # Y = total_data['runtime'].to_numpy()\n",
    "    print('Data X and Y shape', X.shape, Y.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print('Train Test Split:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    # 1. SVR \n",
    "    best_svr = SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    \n",
    "    # 2. LR\n",
    "    best_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "    \n",
    "    # 3. RR\n",
    "    best_rr = linear_model.Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,\n",
    "      random_state=None, solver='lsqr', tol=0.001)\n",
    "    \n",
    "    # 4. KNN\n",
    "    best_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=15, p=1,\n",
    "                    weights='distance')\n",
    "    \n",
    "    # 5. GPR\n",
    "    best_gpr = GaussianProcessRegressor(alpha=0.01, copy_X_train=True, kernel=None,\n",
    "                         n_restarts_optimizer=0, normalize_y=True,\n",
    "                         optimizer='fmin_l_bfgs_b', random_state=None)\n",
    "    # 6. Decision Tree\n",
    "    best_dt = DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
    "                      max_features='sqrt', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=2, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, presort=False,\n",
    "                      random_state=None, splitter='best')\n",
    "    \n",
    "    # 7. Random Forest \n",
    "    best_rf = RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=3,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                      n_jobs=None, oob_score=False, random_state=None,\n",
    "                      verbose=0, warm_start='True')\n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    best_etr = ExtraTreesRegressor(bootstrap=False, criterion='friedman_mse', max_depth=3,\n",
    "                    max_features='auto', max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                    min_samples_leaf=1, min_samples_split=2,\n",
    "                    min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "                    oob_score=False, random_state=42, verbose=0,\n",
    "                    warm_start='True')\n",
    "    # 9. GBR\n",
    "    best_gbr = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='mae', init=None,\n",
    "                          learning_rate=0.1, loss='lad', max_depth=3,\n",
    "                          max_features=None, max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=1, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=42, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "    \n",
    "    # 10. XGB\n",
    "    best_xgb = xgb.XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.5, max_delta_step=0,\n",
    "             max_depth=4, min_child_weight=1, missing=None, n_estimators=10,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, validate_parameters=False, verbosity=1)\n",
    "    \n",
    "    best_models = [best_svr, best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr, best_gbr, best_xgb]\n",
    "    best_models_name = ['best_svr', 'best_lr', 'best_rr', 'best_knn', 'best_gpr', 'best_dt', 'best_rf', 'best_etr'\n",
    "                        , 'best_gbr', 'best_xgb']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mse', 'mape', 'mae' ])\n",
    "    \n",
    "    for model in best_models:\n",
    "        \n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mse_scores = []\n",
    "        mape_scores = []\n",
    "        mae_scores = []\n",
    "\n",
    "        # cv = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "        cv = ShuffleSplit(n_splits=10, random_state=0)\n",
    "        # print(cv)\n",
    "        \n",
    "        fold = 1\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            model_orig = model\n",
    "            # print(\"Train Index: \", train_index, \"\\n\")\n",
    "            # print(\"Test Index: \", test_index)\n",
    "\n",
    "            X_train_fold, X_test_fold, Y_train_fold, Y_test_fold = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "            # print(X_train_fold.shape, X_test_fold.shape, Y_train_fold.shape, Y_test_fold.shape)\n",
    "            model_orig.fit(X_train_fold, Y_train_fold)\n",
    "            Y_pred_fold = model_orig.predict(X_test_fold)\n",
    "            \n",
    "            # save the folds to disk\n",
    "            data = [X_train_fold, X_test_fold, Y_train_fold, Y_test_fold]\n",
    "            filename = path_for_saving_data + '/folds_data/' + best_models_name[k] +'_'+ str(fold) + '.pickle'\n",
    "            pickle.dump(data, open(filename, 'wb'))\n",
    "            \n",
    "            \n",
    "            # save the model to disk\n",
    "            filename = path_for_saving_data + '/models_data/' + best_models_name[k] + '_' + str(fold) + '.sav'\n",
    "            fold = fold + 1\n",
    "            pickle.dump(model_orig, open(filename, 'wb'))\n",
    "\n",
    "            # some time later...\n",
    "            '''\n",
    "            # load the model from disk\n",
    "            loaded_model = pickle.load(open(filename, 'rb'))\n",
    "            result = loaded_model.score(X_test, Y_test)\n",
    "            print(result)\n",
    "            '''\n",
    "            # scores.append(best_svr.score(X_test, y_test))\n",
    "            '''\n",
    "            plt.figure()\n",
    "            plt.plot(Y_test_fold, 'b')\n",
    "            plt.plot(Y_pred_fold, 'r')\n",
    "            '''\n",
    "            # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "            r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "            mse_scores.append(mean_squared_error(Y_test_fold, Y_pred_fold))\n",
    "            mape_scores.append(absolute_percentage_error(Y_test_fold, Y_pred_fold))\n",
    "            mae_scores.append(mean_absolute_error(Y_test_fold, Y_pred_fold))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores, 'mse': mse_scores, 'mape': mape_scores, 'mae': mae_scores }, ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv(path_for_saving_data + '.csv')\n",
    "        # print('MSE for 10 folds\\n', mse_scores)\n",
    "        # print('\\nR2 scores for 10 folds\\n', r2_scores)\n",
    "        # print('\\nMAPE for 10 folds\\n', mape_scores)\n",
    "        # print('\\nMAE scores for 10 folds\\n', mae_scores)\n",
    "        # print('\\nMean MSE = ', np.mean(mse_scores), '\\nMedian MSE = ', np.median(mse_scores))\n",
    "        # print('\\nMean R2 score =',np.mean(r2_scores), '\\nMedian R2 scores = ', np.median(r2_scores))\n",
    "        # print('\\nMean Absolute Percentage Error =',np.mean(mape_scores), \n",
    "        #       '\\nMedian Absolute Percentage Error =', np.median(mape_scores))    \n",
    "        # print('\\nMean MAE =',np.mean(mae_scores), \n",
    "        #      '\\nMedian MAE =', np.median(mae_scores)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'dijkstra_physical'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\\\Performance\\\\PhysicalSystems\\\\dijkstra_physical.csv'\n",
    "path_for_saving_data = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\results_with_normalization_without_PCA\\\\' + dataset_name\n",
    "process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 8 : sha_physical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_sha_physical(dataset_path, dataset_name, path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df, \n",
    "                                                                             encoder_isa = None, encoder_mem_type=None)\n",
    "    # total_data = encoded_data_frame.drop(columns = ['arch', 'arch1'])\n",
    "    \n",
    "    total_data = encoded_data_frame.drop(columns = ['arch'])\n",
    "    total_data = total_data.fillna(0)\n",
    "    X_columns = total_data.drop(columns = 'runtime').columns\n",
    "    X = total_data.drop(columns = ['runtime']).to_numpy()\n",
    "    Y = total_data['runtime'].to_numpy()\n",
    "    # X_columns = total_data.drop(columns = 'PS').columns\n",
    "    # X = total_data.drop(columns = ['runtime','PS']).to_numpy()\n",
    "    # Y = total_data['runtime'].to_numpy()\n",
    "    print('Data X and Y shape', X.shape, Y.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print('Train Test Split:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    # 1. SVR \n",
    "    best_svr = SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    \n",
    "    # 2. LR\n",
    "    best_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=True)\n",
    "    \n",
    "    # 3. RR\n",
    "    best_rr = linear_model.Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,\n",
    "      random_state=None, solver='lsqr', tol=0.001)\n",
    "    \n",
    "    # 4. KNN\n",
    "    best_knn =  KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=13, p=7,\n",
    "                    weights='distance')\n",
    "    \n",
    "    # 5. GPR\n",
    "    best_gpr = GaussianProcessRegressor(alpha=0.01, copy_X_train=True, kernel=None,\n",
    "                         n_restarts_optimizer=0, normalize_y=True,\n",
    "                         optimizer='fmin_l_bfgs_b', random_state=None)\n",
    "    # 6. Decision Tree\n",
    "    best_dt = DecisionTreeRegressor(criterion='friedman_mse', max_depth=15,\n",
    "                      max_features='sqrt', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=2, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, presort=False,\n",
    "                      random_state=None, splitter='random')\n",
    "    \n",
    "    # 7. Random Forest \n",
    "    best_rf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=3,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                      n_jobs=None, oob_score=False, random_state=None,\n",
    "                      verbose=0, warm_start='False')\n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    best_etr = ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=3,\n",
    "                    max_features='auto', max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                    min_samples_leaf=1, min_samples_split=2,\n",
    "                    min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
    "                    oob_score=False, random_state=42, verbose=0,\n",
    "                    warm_start='True')\n",
    "    # 9. GBR\n",
    "    best_gbr = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='mse', init=None,\n",
    "                          learning_rate=0.1, loss='lad', max_depth=5,\n",
    "                          max_features=None, max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=1, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=42, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "    \n",
    "    # 10. XGB\n",
    "    best_xgb = xgb.XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.5, max_delta_step=0,\n",
    "             max_depth=4, min_child_weight=1, missing=None, n_estimators=10,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, validate_parameters=False, verbosity=1)\n",
    "    \n",
    "    best_models = [best_svr, best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr, best_gbr, best_xgb]\n",
    "    best_models_name = ['best_svr', 'best_lr', 'best_rr', 'best_knn', 'best_gpr', 'best_dt', 'best_rf', 'best_etr'\n",
    "                        , 'best_gbr', 'best_xgb']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mse', 'mape', 'mae' ])\n",
    "    \n",
    "    for model in best_models:\n",
    "        \n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mse_scores = []\n",
    "        mape_scores = []\n",
    "        mae_scores = []\n",
    "\n",
    "        # cv = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "        cv = ShuffleSplit(n_splits=10, random_state=0)\n",
    "        # print(cv)\n",
    "        \n",
    "        fold = 1\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            model_orig = model\n",
    "            # print(\"Train Index: \", train_index, \"\\n\")\n",
    "            # print(\"Test Index: \", test_index)\n",
    "\n",
    "            X_train_fold, X_test_fold, Y_train_fold, Y_test_fold = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "            # print(X_train_fold.shape, X_test_fold.shape, Y_train_fold.shape, Y_test_fold.shape)\n",
    "            model_orig.fit(X_train_fold, Y_train_fold)\n",
    "            Y_pred_fold = model_orig.predict(X_test_fold)\n",
    "            \n",
    "            # save the folds to disk\n",
    "            data = [X_train_fold, X_test_fold, Y_train_fold, Y_test_fold]\n",
    "            filename = path_for_saving_data + '/folds_data/' + best_models_name[k] +'_'+ str(fold) + '.pickle'\n",
    "            pickle.dump(data, open(filename, 'wb'))\n",
    "            \n",
    "            \n",
    "            # save the model to disk\n",
    "            filename = path_for_saving_data + '/models_data/' + best_models_name[k] + '_' + str(fold) + '.sav'\n",
    "            fold = fold + 1\n",
    "            pickle.dump(model_orig, open(filename, 'wb'))\n",
    "\n",
    "            # some time later...\n",
    "            '''\n",
    "            # load the model from disk\n",
    "            loaded_model = pickle.load(open(filename, 'rb'))\n",
    "            result = loaded_model.score(X_test, Y_test)\n",
    "            print(result)\n",
    "            '''\n",
    "            # scores.append(best_svr.score(X_test, y_test))\n",
    "            '''\n",
    "            plt.figure()\n",
    "            plt.plot(Y_test_fold, 'b')\n",
    "            plt.plot(Y_pred_fold, 'r')\n",
    "            '''\n",
    "            # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "            r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "            mse_scores.append(mean_squared_error(Y_test_fold, Y_pred_fold))\n",
    "            mape_scores.append(absolute_percentage_error(Y_test_fold, Y_pred_fold))\n",
    "            mae_scores.append(mean_absolute_error(Y_test_fold, Y_pred_fold))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores, 'mse': mse_scores, 'mape': mape_scores, 'mae': mae_scores }, ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv(path_for_saving_data + '.csv')\n",
    "        # print('MSE for 10 folds\\n', mse_scores)\n",
    "        # print('\\nR2 scores for 10 folds\\n', r2_scores)\n",
    "        # print('\\nMAPE for 10 folds\\n', mape_scores)\n",
    "        # print('\\nMAE scores for 10 folds\\n', mae_scores)\n",
    "        # print('\\nMean MSE = ', np.mean(mse_scores), '\\nMedian MSE = ', np.median(mse_scores))\n",
    "        # print('\\nMean R2 score =',np.mean(r2_scores), '\\nMedian R2 scores = ', np.median(r2_scores))\n",
    "        # print('\\nMean Absolute Percentage Error =',np.mean(mape_scores), \n",
    "        #       '\\nMedian Absolute Percentage Error =', np.median(mape_scores))    \n",
    "        # print('\\nMean MAE =',np.mean(mae_scores), \n",
    "        #      '\\nMedian MAE =', np.median(mae_scores)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data X and Y shape (52, 20) (52,)\n",
      "Train Test Split: (41, 20) (11, 20) (41,) (11,)\n",
      "Running model number: 1 with Model Name:  best_svr\n",
      "Running model number: 2 with Model Name:  best_lr\n",
      "Running model number: 3 with Model Name:  best_rr\n",
      "Running model number: 4 with Model Name:  best_knn\n",
      "Running model number: 5 with Model Name:  best_gpr\n",
      "Running model number: 6 with Model Name:  best_dt\n",
      "Running model number: 7 with Model Name:  best_rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model number: 8 with Model Name:  best_etr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model number: 9 with Model Name:  best_gbr\n",
      "Running model number: 10 with Model Name:  best_xgb\n",
      "[16:26:04] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:26:04] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:26:04] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:26:04] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:26:04] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:26:04] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:26:04] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:26:04] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:26:04] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:26:04] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  model_name  dataset_name                                                 r2  \\\n",
      "0   best_svr  sha_physical  [0.5439399158414611, 0.12855304675271628, 0.22...   \n",
      "1    best_lr  sha_physical  [0.8294903913494944, 0.3970132969485334, 0.396...   \n",
      "2    best_rr  sha_physical  [0.8029926849948819, 0.1595873851196461, 0.484...   \n",
      "3   best_knn  sha_physical  [0.8139163449771692, 0.30931401585212936, 0.37...   \n",
      "4   best_gpr  sha_physical  [0.5302629303915782, -0.7435340173000458, 0.09...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [15007066.046998067, 35576553.14683912, 211766...   \n",
      "1  [5610771.579335157, 24616746.21502805, 1659621...   \n",
      "2  [6482702.369093542, 34309585.852761306, 141749...   \n",
      "3  [6123249.541443429, 28197042.32282849, 1711098...   \n",
      "4  [15457119.518239526, 71179238.61992726, 248968...   \n",
      "\n",
      "                                                mape  \\\n",
      "0  [0.13582241805606096, 0.15921148387944145, 0.1...   \n",
      "1  [0.04000932564609063, 0.12318346140113924, 0.0...   \n",
      "2  [0.04954799909701917, 0.14050847057303711, 0.1...   \n",
      "3  [0.04050667609657876, 0.11842656280322784, 0.0...   \n",
      "4  [0.08004696249659292, 0.19148375335442383, 0.1...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [2812.221900778095, 4401.885142305119, 3378.32...  \n",
      "1  [1179.007416666667, 3701.162366666667, 2667.77...  \n",
      "2  [1358.0795087202039, 4231.188805209005, 3025.8...  \n",
      "3  [1197.1320558752905, 3670.6182504323583, 2760....  \n",
      "4  [2253.035775326823, 5893.440778813428, 3262.19...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'sha_physical'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\\\Performance\\\\PhysicalSystems\\\\sha_physical.csv'\n",
    "path_for_saving_data = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\results_with_normalization_without_PCA\\\\' + dataset_name\n",
    "process_all_sha_physical(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 9 : sha_simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df, \n",
    "                                                                             encoder_isa = None, encoder_mem_type=None)\n",
    "    # total_data = encoded_data_frame.drop(columns = ['arch', 'arch1'])\n",
    "    \n",
    "    total_data = encoded_data_frame.drop(columns = ['arch'])\n",
    "    total_data = total_data.fillna(0)\n",
    "    X_columns = total_data.drop(columns = 'runtime').columns\n",
    "    X = total_data.drop(columns = ['runtime']).to_numpy()\n",
    "    Y = total_data['runtime'].to_numpy()\n",
    "    # X_columns = total_data.drop(columns = 'PS').columns\n",
    "    # X = total_data.drop(columns = ['runtime','PS']).to_numpy()\n",
    "    # Y = total_data['runtime'].to_numpy()\n",
    "    print('Data X and Y shape', X.shape, Y.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print('Train Test Split:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    # 1. SVR \n",
    "    best_svr = SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    \n",
    "    # 2. LR\n",
    "    best_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "    \n",
    "    # 3. RR\n",
    "    best_rr = linear_model.Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,\n",
    "      random_state=None, solver='lsqr', tol=0.001)\n",
    "    \n",
    "    # 4. KNN\n",
    "    best_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=15, p=1,\n",
    "                    weights='distance')\n",
    "    \n",
    "    # 5. GPR\n",
    "    best_gpr = GaussianProcessRegressor(alpha=0.01, copy_X_train=True, kernel=None,\n",
    "                         n_restarts_optimizer=0, normalize_y=True,\n",
    "                         optimizer='fmin_l_bfgs_b', random_state=None)\n",
    "    # 6. Decision Tree\n",
    "    best_dt = DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
    "                      max_features='sqrt', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=2, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, presort=False,\n",
    "                      random_state=None, splitter='best')\n",
    "    \n",
    "    # 7. Random Forest \n",
    "    best_rf = RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=3,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                      n_jobs=None, oob_score=False, random_state=None,\n",
    "                      verbose=0, warm_start='True')\n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    best_etr = ExtraTreesRegressor(bootstrap=False, criterion='friedman_mse', max_depth=3,\n",
    "                    max_features='auto', max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                    min_samples_leaf=1, min_samples_split=2,\n",
    "                    min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "                    oob_score=False, random_state=42, verbose=0,\n",
    "                    warm_start='True')\n",
    "    # 9. GBR\n",
    "    best_gbr = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='mae', init=None,\n",
    "                          learning_rate=0.1, loss='lad', max_depth=3,\n",
    "                          max_features=None, max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=1, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=42, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "    \n",
    "    # 10. XGB\n",
    "    best_xgb = xgb.XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.5, max_delta_step=0,\n",
    "             max_depth=4, min_child_weight=1, missing=None, n_estimators=10,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, validate_parameters=False, verbosity=1)\n",
    "    \n",
    "    best_models = [best_svr, best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr, best_gbr, best_xgb]\n",
    "    best_models_name = ['best_svr', 'best_lr', 'best_rr', 'best_knn', 'best_gpr', 'best_dt', 'best_rf', 'best_etr'\n",
    "                        , 'best_gbr', 'best_xgb']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mse', 'mape', 'mae' ])\n",
    "    \n",
    "    for model in best_models:\n",
    "        \n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mse_scores = []\n",
    "        mape_scores = []\n",
    "        mae_scores = []\n",
    "\n",
    "        # cv = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "        cv = ShuffleSplit(n_splits=10, random_state=0)\n",
    "        # print(cv)\n",
    "        \n",
    "        fold = 1\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            model_orig = model\n",
    "            # print(\"Train Index: \", train_index, \"\\n\")\n",
    "            # print(\"Test Index: \", test_index)\n",
    "\n",
    "            X_train_fold, X_test_fold, Y_train_fold, Y_test_fold = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "            # print(X_train_fold.shape, X_test_fold.shape, Y_train_fold.shape, Y_test_fold.shape)\n",
    "            model_orig.fit(X_train_fold, Y_train_fold)\n",
    "            Y_pred_fold = model_orig.predict(X_test_fold)\n",
    "            \n",
    "            # save the folds to disk\n",
    "            data = [X_train_fold, X_test_fold, Y_train_fold, Y_test_fold]\n",
    "            filename = path_for_saving_data + '/folds_data/' + best_models_name[k] +'_'+ str(fold) + '.pickle'\n",
    "            pickle.dump(data, open(filename, 'wb'))\n",
    "            \n",
    "            \n",
    "            # save the model to disk\n",
    "            filename = path_for_saving_data + '/models_data/' + best_models_name[k] + '_' + str(fold) + '.sav'\n",
    "            fold = fold + 1\n",
    "            pickle.dump(model_orig, open(filename, 'wb'))\n",
    "\n",
    "            # some time later...\n",
    "            '''\n",
    "            # load the model from disk\n",
    "            loaded_model = pickle.load(open(filename, 'rb'))\n",
    "            result = loaded_model.score(X_test, Y_test)\n",
    "            print(result)\n",
    "            '''\n",
    "            # scores.append(best_svr.score(X_test, y_test))\n",
    "            '''\n",
    "            plt.figure()\n",
    "            plt.plot(Y_test_fold, 'b')\n",
    "            plt.plot(Y_pred_fold, 'r')\n",
    "            '''\n",
    "            # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "            r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "            mse_scores.append(mean_squared_error(Y_test_fold, Y_pred_fold))\n",
    "            mape_scores.append(absolute_percentage_error(Y_test_fold, Y_pred_fold))\n",
    "            mae_scores.append(mean_absolute_error(Y_test_fold, Y_pred_fold))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores, 'mse': mse_scores, 'mape': mape_scores, 'mae': mae_scores }, ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv(path_for_saving_data + '.csv')\n",
    "        # print('MSE for 10 folds\\n', mse_scores)\n",
    "        # print('\\nR2 scores for 10 folds\\n', r2_scores)\n",
    "        # print('\\nMAPE for 10 folds\\n', mape_scores)\n",
    "        # print('\\nMAE scores for 10 folds\\n', mae_scores)\n",
    "        # print('\\nMean MSE = ', np.mean(mse_scores), '\\nMedian MSE = ', np.median(mse_scores))\n",
    "        # print('\\nMean R2 score =',np.mean(r2_scores), '\\nMedian R2 scores = ', np.median(r2_scores))\n",
    "        # print('\\nMean Absolute Percentage Error =',np.mean(mape_scores), \n",
    "        #       '\\nMedian Absolute Percentage Error =', np.median(mape_scores))    \n",
    "        # print('\\nMean MAE =',np.mean(mae_scores), \n",
    "        #      '\\nMedian MAE =', np.median(mae_scores)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'dijkstra_physical'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\\\Performance\\\\PhysicalSystems\\\\dijkstra_physical.csv'\n",
    "path_for_saving_data = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\results_with_normalization_without_PCA\\\\' + dataset_name\n",
    "process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 10 : stitch_physical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_stitch_physical(dataset_path, dataset_name, path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df, \n",
    "                                                                             encoder_isa = None, encoder_mem_type=None)\n",
    "    # total_data = encoded_data_frame.drop(columns = ['arch', 'arch1'])\n",
    "    \n",
    "    total_data = encoded_data_frame.drop(columns = ['arch'])\n",
    "    total_data = total_data.fillna(0)\n",
    "    X_columns = total_data.drop(columns = 'runtime').columns\n",
    "    X = total_data.drop(columns = ['runtime']).to_numpy()\n",
    "    Y = total_data['runtime'].to_numpy()\n",
    "    # X_columns = total_data.drop(columns = 'PS').columns\n",
    "    # X = total_data.drop(columns = ['runtime','PS']).to_numpy()\n",
    "    # Y = total_data['runtime'].to_numpy()\n",
    "    print('Data X and Y shape', X.shape, Y.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print('Train Test Split:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    # 1. SVR \n",
    "    best_svr = SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    \n",
    "    # 2. LR\n",
    "    best_lr =  LinearRegression(copy_X=True, fit_intercept=False, n_jobs=None, normalize=True)\n",
    "    \n",
    "    # 3. RR\n",
    "    best_rr = linear_model.Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None, normalize=True,\n",
    "      random_state=None, solver='sparse_cg', tol=0.001)\n",
    "    \n",
    "    # 4. KNN\n",
    "    best_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=13, p=4,\n",
    "                    weights='distance')\n",
    "    \n",
    "    # 5. GPR\n",
    "    best_gpr = GaussianProcessRegressor(alpha=0.01, copy_X_train=True, kernel=None,\n",
    "                         n_restarts_optimizer=0, normalize_y=False,\n",
    "                         optimizer='fmin_l_bfgs_b', random_state=None)\n",
    "    # 6. Decision Tree\n",
    "    best_dt = DecisionTreeRegressor(criterion='mae', max_depth=9, max_features='sqrt',\n",
    "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                      min_impurity_split=None, min_samples_leaf=1,\n",
    "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                      presort=False, random_state=None, splitter='random')\n",
    "    \n",
    "    # 7. Random Forest \n",
    "    best_rf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=4,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=10,\n",
    "                      n_jobs=None, oob_score=False, random_state=None,\n",
    "                      verbose=0, warm_start='False')\n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    best_etr = ExtraTreesRegressor(bootstrap=False, criterion='friedman_mse', max_depth=4,\n",
    "                    max_features='auto', max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                    min_samples_leaf=1, min_samples_split=2,\n",
    "                    min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
    "                    oob_score=False, random_state=42, verbose=0,\n",
    "                    warm_start='True')\n",
    "    # 9. GBR\n",
    "    best_gbr = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='mse', init=None,\n",
    "                          learning_rate=0.1, loss='lad', max_depth=5,\n",
    "                          max_features=None, max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=1, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=42, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "    \n",
    "    # 10. XGB\n",
    "    best_xgb = xgb.XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.5, max_delta_step=0,\n",
    "             max_depth=4, min_child_weight=1, missing=None, n_estimators=10,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, validate_parameters=False, verbosity=1)\n",
    "    \n",
    "    best_models = [best_svr, best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr, best_gbr, best_xgb]\n",
    "    best_models_name = ['best_svr', 'best_lr', 'best_rr', 'best_knn', 'best_gpr', 'best_dt', 'best_rf', 'best_etr'\n",
    "                        , 'best_gbr', 'best_xgb']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mse', 'mape', 'mae' ])\n",
    "    \n",
    "    for model in best_models:\n",
    "        \n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mse_scores = []\n",
    "        mape_scores = []\n",
    "        mae_scores = []\n",
    "\n",
    "        # cv = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "        cv = ShuffleSplit(n_splits=10, random_state=0)\n",
    "        # print(cv)\n",
    "        \n",
    "        fold = 1\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            model_orig = model\n",
    "            # print(\"Train Index: \", train_index, \"\\n\")\n",
    "            # print(\"Test Index: \", test_index)\n",
    "\n",
    "            X_train_fold, X_test_fold, Y_train_fold, Y_test_fold = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "            # print(X_train_fold.shape, X_test_fold.shape, Y_train_fold.shape, Y_test_fold.shape)\n",
    "            model_orig.fit(X_train_fold, Y_train_fold)\n",
    "            Y_pred_fold = model_orig.predict(X_test_fold)\n",
    "            \n",
    "            # save the folds to disk\n",
    "            data = [X_train_fold, X_test_fold, Y_train_fold, Y_test_fold]\n",
    "            filename = path_for_saving_data + '/folds_data/' + best_models_name[k] +'_'+ str(fold) + '.pickle'\n",
    "            pickle.dump(data, open(filename, 'wb'))\n",
    "            \n",
    "            \n",
    "            # save the model to disk\n",
    "            filename = path_for_saving_data + '/models_data/' + best_models_name[k] + '_' + str(fold) + '.sav'\n",
    "            fold = fold + 1\n",
    "            pickle.dump(model_orig, open(filename, 'wb'))\n",
    "\n",
    "            # some time later...\n",
    "            '''\n",
    "            # load the model from disk\n",
    "            loaded_model = pickle.load(open(filename, 'rb'))\n",
    "            result = loaded_model.score(X_test, Y_test)\n",
    "            print(result)\n",
    "            '''\n",
    "            # scores.append(best_svr.score(X_test, y_test))\n",
    "            '''\n",
    "            plt.figure()\n",
    "            plt.plot(Y_test_fold, 'b')\n",
    "            plt.plot(Y_pred_fold, 'r')\n",
    "            '''\n",
    "            # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "            r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "            mse_scores.append(mean_squared_error(Y_test_fold, Y_pred_fold))\n",
    "            mape_scores.append(absolute_percentage_error(Y_test_fold, Y_pred_fold))\n",
    "            mae_scores.append(mean_absolute_error(Y_test_fold, Y_pred_fold))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores, 'mse': mse_scores, 'mape': mape_scores, 'mae': mae_scores }, ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv(path_for_saving_data + '.csv')\n",
    "        # print('MSE for 10 folds\\n', mse_scores)\n",
    "        # print('\\nR2 scores for 10 folds\\n', r2_scores)\n",
    "        # print('\\nMAPE for 10 folds\\n', mape_scores)\n",
    "        # print('\\nMAE scores for 10 folds\\n', mae_scores)\n",
    "        # print('\\nMean MSE = ', np.mean(mse_scores), '\\nMedian MSE = ', np.median(mse_scores))\n",
    "        # print('\\nMean R2 score =',np.mean(r2_scores), '\\nMedian R2 scores = ', np.median(r2_scores))\n",
    "        # print('\\nMean Absolute Percentage Error =',np.mean(mape_scores), \n",
    "        #       '\\nMedian Absolute Percentage Error =', np.median(mape_scores))    \n",
    "        # print('\\nMean MAE =',np.mean(mae_scores), \n",
    "        #      '\\nMedian MAE =', np.median(mae_scores)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data X and Y shape (52, 20) (52,)\n",
      "Train Test Split: (41, 20) (11, 20) (41,) (11,)\n",
      "Running model number: 1 with Model Name:  best_svr\n",
      "Running model number: 2 with Model Name:  best_lr\n",
      "Running model number: 3 with Model Name:  best_rr\n",
      "Running model number: 4 with Model Name:  best_knn\n",
      "Running model number: 5 with Model Name:  best_gpr\n",
      "Running model number: 6 with Model Name:  best_dt\n",
      "Running model number: 7 with Model Name:  best_rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model number: 8 with Model Name:  best_etr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model number: 9 with Model Name:  best_gbr\n",
      "Running model number: 10 with Model Name:  best_xgb\n",
      "[16:38:01] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:38:01] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:38:01] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:38:01] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:38:01] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:38:01] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:38:01] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:38:01] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:38:01] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:38:01] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  model_name     dataset_name  \\\n",
      "0   best_svr  stitch_physical   \n",
      "1    best_lr  stitch_physical   \n",
      "2    best_rr  stitch_physical   \n",
      "3   best_knn  stitch_physical   \n",
      "4   best_gpr  stitch_physical   \n",
      "\n",
      "                                                  r2  \\\n",
      "0  [0.6949980747330784, 0.6246862331067944, 0.643...   \n",
      "1  [0.8226182587873474, 0.5269882673816201, 0.669...   \n",
      "2  [0.8226075574918322, 0.7962183000617079, 0.704...   \n",
      "3  [0.8041550111102402, 0.7326424008143987, 0.644...   \n",
      "4  [0.4469629764218257, -1.850601378988788, -8.81...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [7607569.041533611, 5397689.546858382, 3536098...   \n",
      "1  [4424378.114340558, 6802762.674629249, 3282864...   \n",
      "2  [4424645.033457197, 2930748.745784586, 2933279...   \n",
      "3  [4884901.212060691, 3845084.954765081, 3526902...   \n",
      "4  [13794232.071529493, 40996794.21033895, 973984...   \n",
      "\n",
      "                                                mape  \\\n",
      "0  [0.12952597292757073, 0.10778143684976366, 0.0...   \n",
      "1  [0.05838108542059552, 0.08277109920129283, 0.0...   \n",
      "2  [0.05942817685484225, 0.06514287641502797, 0.0...   \n",
      "3  [0.06603989942779591, 0.07142907744881076, 0.0...   \n",
      "4  [0.12139357885852163, 0.19395303911049144, 0.3...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [2340.343132422349, 2104.9810737135085, 1618.9...  \n",
      "1  [1338.425920377548, 1781.1766733320471, 1253.4...  \n",
      "2  [1354.4450614071854, 1397.3768364087093, 1063....  \n",
      "3  [1511.7751336666413, 1516.6160514035273, 1416....  \n",
      "4  [2775.7021512469732, 4054.449295463984, 7343.1...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'stitch_physical'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\\\Performance\\\\PhysicalSystems\\\\stitch_physical.csv'\n",
    "path_for_saving_data = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\results_with_normalization_without_PCA\\\\' + dataset_name\n",
    "process_all_stitch_physical(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 11 : stitch_simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df, \n",
    "                                                                             encoder_isa = None, encoder_mem_type=None)\n",
    "    # total_data = encoded_data_frame.drop(columns = ['arch', 'arch1'])\n",
    "    \n",
    "    total_data = encoded_data_frame.drop(columns = ['arch'])\n",
    "    total_data = total_data.fillna(0)\n",
    "    X_columns = total_data.drop(columns = 'runtime').columns\n",
    "    X = total_data.drop(columns = ['runtime']).to_numpy()\n",
    "    Y = total_data['runtime'].to_numpy()\n",
    "    # X_columns = total_data.drop(columns = 'PS').columns\n",
    "    # X = total_data.drop(columns = ['runtime','PS']).to_numpy()\n",
    "    # Y = total_data['runtime'].to_numpy()\n",
    "    print('Data X and Y shape', X.shape, Y.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print('Train Test Split:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    # 1. SVR \n",
    "    best_svr = SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    \n",
    "    # 2. LR\n",
    "    best_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "    \n",
    "    # 3. RR\n",
    "    best_rr = linear_model.Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,\n",
    "      random_state=None, solver='lsqr', tol=0.001)\n",
    "    \n",
    "    # 4. KNN\n",
    "    best_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=15, p=1,\n",
    "                    weights='distance')\n",
    "    \n",
    "    # 5. GPR\n",
    "    best_gpr = GaussianProcessRegressor(alpha=0.01, copy_X_train=True, kernel=None,\n",
    "                         n_restarts_optimizer=0, normalize_y=True,\n",
    "                         optimizer='fmin_l_bfgs_b', random_state=None)\n",
    "    # 6. Decision Tree\n",
    "    best_dt = DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
    "                      max_features='sqrt', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=2, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, presort=False,\n",
    "                      random_state=None, splitter='best')\n",
    "    \n",
    "    # 7. Random Forest \n",
    "    best_rf = RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=3,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                      n_jobs=None, oob_score=False, random_state=None,\n",
    "                      verbose=0, warm_start='True')\n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    best_etr = ExtraTreesRegressor(bootstrap=False, criterion='friedman_mse', max_depth=3,\n",
    "                    max_features='auto', max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                    min_samples_leaf=1, min_samples_split=2,\n",
    "                    min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "                    oob_score=False, random_state=42, verbose=0,\n",
    "                    warm_start='True')\n",
    "    # 9. GBR\n",
    "    best_gbr = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='mae', init=None,\n",
    "                          learning_rate=0.1, loss='lad', max_depth=3,\n",
    "                          max_features=None, max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=1, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=42, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "    \n",
    "    # 10. XGB\n",
    "    best_xgb = xgb.XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.5, max_delta_step=0,\n",
    "             max_depth=4, min_child_weight=1, missing=None, n_estimators=10,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, validate_parameters=False, verbosity=1)\n",
    "    \n",
    "    best_models = [best_svr, best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr, best_gbr, best_xgb]\n",
    "    best_models_name = ['best_svr', 'best_lr', 'best_rr', 'best_knn', 'best_gpr', 'best_dt', 'best_rf', 'best_etr'\n",
    "                        , 'best_gbr', 'best_xgb']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mse', 'mape', 'mae' ])\n",
    "    \n",
    "    for model in best_models:\n",
    "        \n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mse_scores = []\n",
    "        mape_scores = []\n",
    "        mae_scores = []\n",
    "\n",
    "        # cv = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "        cv = ShuffleSplit(n_splits=10, random_state=0)\n",
    "        # print(cv)\n",
    "        \n",
    "        fold = 1\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            model_orig = model\n",
    "            # print(\"Train Index: \", train_index, \"\\n\")\n",
    "            # print(\"Test Index: \", test_index)\n",
    "\n",
    "            X_train_fold, X_test_fold, Y_train_fold, Y_test_fold = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "            # print(X_train_fold.shape, X_test_fold.shape, Y_train_fold.shape, Y_test_fold.shape)\n",
    "            model_orig.fit(X_train_fold, Y_train_fold)\n",
    "            Y_pred_fold = model_orig.predict(X_test_fold)\n",
    "            \n",
    "            # save the folds to disk\n",
    "            data = [X_train_fold, X_test_fold, Y_train_fold, Y_test_fold]\n",
    "            filename = path_for_saving_data + '/folds_data/' + best_models_name[k] +'_'+ str(fold) + '.pickle'\n",
    "            pickle.dump(data, open(filename, 'wb'))\n",
    "            \n",
    "            \n",
    "            # save the model to disk\n",
    "            filename = path_for_saving_data + '/models_data/' + best_models_name[k] + '_' + str(fold) + '.sav'\n",
    "            fold = fold + 1\n",
    "            pickle.dump(model_orig, open(filename, 'wb'))\n",
    "\n",
    "            # some time later...\n",
    "            '''\n",
    "            # load the model from disk\n",
    "            loaded_model = pickle.load(open(filename, 'rb'))\n",
    "            result = loaded_model.score(X_test, Y_test)\n",
    "            print(result)\n",
    "            '''\n",
    "            # scores.append(best_svr.score(X_test, y_test))\n",
    "            '''\n",
    "            plt.figure()\n",
    "            plt.plot(Y_test_fold, 'b')\n",
    "            plt.plot(Y_pred_fold, 'r')\n",
    "            '''\n",
    "            # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "            r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "            mse_scores.append(mean_squared_error(Y_test_fold, Y_pred_fold))\n",
    "            mape_scores.append(absolute_percentage_error(Y_test_fold, Y_pred_fold))\n",
    "            mae_scores.append(mean_absolute_error(Y_test_fold, Y_pred_fold))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores, 'mse': mse_scores, 'mape': mape_scores, 'mae': mae_scores }, ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv(path_for_saving_data + '.csv')\n",
    "        # print('MSE for 10 folds\\n', mse_scores)\n",
    "        # print('\\nR2 scores for 10 folds\\n', r2_scores)\n",
    "        # print('\\nMAPE for 10 folds\\n', mape_scores)\n",
    "        # print('\\nMAE scores for 10 folds\\n', mae_scores)\n",
    "        # print('\\nMean MSE = ', np.mean(mse_scores), '\\nMedian MSE = ', np.median(mse_scores))\n",
    "        # print('\\nMean R2 score =',np.mean(r2_scores), '\\nMedian R2 scores = ', np.median(r2_scores))\n",
    "        # print('\\nMean Absolute Percentage Error =',np.mean(mape_scores), \n",
    "        #       '\\nMedian Absolute Percentage Error =', np.median(mape_scores))    \n",
    "        # print('\\nMean MAE =',np.mean(mae_scores), \n",
    "        #      '\\nMedian MAE =', np.median(mae_scores)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'dijkstra_physical'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\\\Performance\\\\PhysicalSystems\\\\dijkstra_physical.csv'\n",
    "path_for_saving_data = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\results_with_normalization_without_PCA\\\\' + dataset_name\n",
    "process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 12 : svm_physical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_svm_physical(dataset_path, dataset_name, path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df, \n",
    "                                                                             encoder_isa = None, encoder_mem_type=None)\n",
    "    # total_data = encoded_data_frame.drop(columns = ['arch', 'arch1'])\n",
    "    \n",
    "    total_data = encoded_data_frame.drop(columns = ['arch'])\n",
    "    total_data = total_data.fillna(0)\n",
    "    X_columns = total_data.drop(columns = 'runtime').columns\n",
    "    X = total_data.drop(columns = ['runtime']).to_numpy()\n",
    "    Y = total_data['runtime'].to_numpy()\n",
    "    # X_columns = total_data.drop(columns = 'PS').columns\n",
    "    # X = total_data.drop(columns = ['runtime','PS']).to_numpy()\n",
    "    # Y = total_data['runtime'].to_numpy()\n",
    "    print('Data X and Y shape', X.shape, Y.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print('Train Test Split:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    # 1. SVR \n",
    "    best_svr = SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    \n",
    "    # 2. LR\n",
    "    best_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "    \n",
    "    # 3. RR\n",
    "    best_rr = linear_model.Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "      normalize=False, random_state=None, solver='sparse_cg', tol=0.001)\n",
    "    \n",
    "    # 4. KNN\n",
    "    best_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=2, p=1,\n",
    "                    weights='distance')\n",
    "    \n",
    "    # 5. GPR\n",
    "    best_gpr = GaussianProcessRegressor(alpha=0.01, copy_X_train=True, kernel=None,\n",
    "                         n_restarts_optimizer=0, normalize_y=True,\n",
    "                         optimizer='fmin_l_bfgs_b', random_state=None)\n",
    "    # 6. Decision Tree\n",
    "    best_dt = DecisionTreeRegressor(criterion='mse', max_depth=5, max_features='log2',\n",
    "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                      min_impurity_split=None, min_samples_leaf=1,\n",
    "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                      presort=False, random_state=None, splitter='random')\n",
    "    \n",
    "    # 7. Random Forest \n",
    "    best_rf = RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=4,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=10,\n",
    "                      n_jobs=None, oob_score=False, random_state=None,\n",
    "                      verbose=0, warm_start='True')\n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    best_etr = ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=5,\n",
    "                    max_features='auto', max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                    min_samples_leaf=1, min_samples_split=2,\n",
    "                    min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "                    oob_score=False, random_state=42, verbose=0,\n",
    "                    warm_start='True')\n",
    "    # 9. GBR\n",
    "    best_gbr = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='mae', init=None,\n",
    "                          learning_rate=0.1, loss='lad', max_depth=3,\n",
    "                          max_features=None, max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=1, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=42, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "    \n",
    "    # 10. XGB\n",
    "    best_xgb = xgb.XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.5, max_delta_step=0,\n",
    "             max_depth=4, min_child_weight=1, missing=None, n_estimators=10,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, validate_parameters=False, verbosity=1)\n",
    "    \n",
    "    best_models = [best_svr, best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr, best_gbr, best_xgb]\n",
    "    best_models_name = ['best_svr', 'best_lr', 'best_rr', 'best_knn', 'best_gpr', 'best_dt', 'best_rf', 'best_etr'\n",
    "                        , 'best_gbr', 'best_xgb']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mse', 'mape', 'mae' ])\n",
    "    \n",
    "    for model in best_models:\n",
    "        \n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mse_scores = []\n",
    "        mape_scores = []\n",
    "        mae_scores = []\n",
    "\n",
    "        # cv = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "        cv = ShuffleSplit(n_splits=10, random_state=0)\n",
    "        # print(cv)\n",
    "        \n",
    "        fold = 1\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            model_orig = model\n",
    "            # print(\"Train Index: \", train_index, \"\\n\")\n",
    "            # print(\"Test Index: \", test_index)\n",
    "\n",
    "            X_train_fold, X_test_fold, Y_train_fold, Y_test_fold = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "            # print(X_train_fold.shape, X_test_fold.shape, Y_train_fold.shape, Y_test_fold.shape)\n",
    "            model_orig.fit(X_train_fold, Y_train_fold)\n",
    "            Y_pred_fold = model_orig.predict(X_test_fold)\n",
    "            \n",
    "            # save the folds to disk\n",
    "            data = [X_train_fold, X_test_fold, Y_train_fold, Y_test_fold]\n",
    "            filename = path_for_saving_data + '/folds_data/' + best_models_name[k] +'_'+ str(fold) + '.pickle'\n",
    "            pickle.dump(data, open(filename, 'wb'))\n",
    "            \n",
    "            \n",
    "            # save the model to disk\n",
    "            filename = path_for_saving_data + '/models_data/' + best_models_name[k] + '_' + str(fold) + '.sav'\n",
    "            fold = fold + 1\n",
    "            pickle.dump(model_orig, open(filename, 'wb'))\n",
    "\n",
    "            # some time later...\n",
    "            '''\n",
    "            # load the model from disk\n",
    "            loaded_model = pickle.load(open(filename, 'rb'))\n",
    "            result = loaded_model.score(X_test, Y_test)\n",
    "            print(result)\n",
    "            '''\n",
    "            # scores.append(best_svr.score(X_test, y_test))\n",
    "            '''\n",
    "            plt.figure()\n",
    "            plt.plot(Y_test_fold, 'b')\n",
    "            plt.plot(Y_pred_fold, 'r')\n",
    "            '''\n",
    "            # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "            r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "            mse_scores.append(mean_squared_error(Y_test_fold, Y_pred_fold))\n",
    "            mape_scores.append(absolute_percentage_error(Y_test_fold, Y_pred_fold))\n",
    "            mae_scores.append(mean_absolute_error(Y_test_fold, Y_pred_fold))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores, 'mse': mse_scores, 'mape': mape_scores, 'mae': mae_scores }, ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv(path_for_saving_data + '.csv')\n",
    "        # print('MSE for 10 folds\\n', mse_scores)\n",
    "        # print('\\nR2 scores for 10 folds\\n', r2_scores)\n",
    "        # print('\\nMAPE for 10 folds\\n', mape_scores)\n",
    "        # print('\\nMAE scores for 10 folds\\n', mae_scores)\n",
    "        # print('\\nMean MSE = ', np.mean(mse_scores), '\\nMedian MSE = ', np.median(mse_scores))\n",
    "        # print('\\nMean R2 score =',np.mean(r2_scores), '\\nMedian R2 scores = ', np.median(r2_scores))\n",
    "        # print('\\nMean Absolute Percentage Error =',np.mean(mape_scores), \n",
    "        #       '\\nMedian Absolute Percentage Error =', np.median(mape_scores))    \n",
    "        # print('\\nMean MAE =',np.mean(mae_scores), \n",
    "        #      '\\nMedian MAE =', np.median(mae_scores)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data X and Y shape (52, 20) (52,)\n",
      "Train Test Split: (41, 20) (11, 20) (41,) (11,)\n",
      "Running model number: 1 with Model Name:  best_svr\n",
      "Running model number: 2 with Model Name:  best_lr\n",
      "Running model number: 3 with Model Name:  best_rr\n",
      "Running model number: 4 with Model Name:  best_knn\n",
      "Running model number: 5 with Model Name:  best_gpr\n",
      "Running model number: 6 with Model Name:  best_dt\n",
      "Running model number: 7 with Model Name:  best_rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model number: 8 with Model Name:  best_etr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "C:\\Users\\Rajat\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:307: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model number: 9 with Model Name:  best_gbr\n",
      "Running model number: 10 with Model Name:  best_xgb\n",
      "[16:43:43] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:43:43] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:43:43] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:43:43] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:43:43] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:43:43] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:43:43] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:43:43] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:43:43] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:43:43] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  model_name  dataset_name                                                 r2  \\\n",
      "0   best_svr  svm_physical  [0.3860817843050499, 0.606153679853983, 0.3428...   \n",
      "1    best_lr  svm_physical  [0.99922632636046, 0.9303403948208646, 0.82499...   \n",
      "2    best_rr  svm_physical  [0.9992812875795799, 0.9049200948394939, 0.874...   \n",
      "3   best_knn  svm_physical  [0.9993418975996776, 0.966790584149034, 0.8341...   \n",
      "4   best_gpr  svm_physical  [0.9955208050804074, 0.9170310605827265, 0.345...   \n",
      "\n",
      "                                                 mse  \\\n",
      "0  [11919165.678956317, 8708334.46592358, 2411410...   \n",
      "1  [15020.802535854325, 1540243.2614812588, 64217...   \n",
      "2  [13953.735522920819, 2102311.416338758, 459216...   \n",
      "3  [12776.997558675437, 734293.2658122513, 608654...   \n",
      "4  [86963.15729046887, 1834525.8994947465, 240030...   \n",
      "\n",
      "                                                mape  \\\n",
      "0  [0.06763686746977163, 0.061695367158961706, 0....   \n",
      "1  [0.0033507977663494912, 0.019199848794019928, ...   \n",
      "2  [0.0026794946320953324, 0.024204969400516266, ...   \n",
      "3  [0.0027465006002319373, 0.013120463375396548, ...   \n",
      "4  [0.0070513467244557254, 0.022267067313777828, ...   \n",
      "\n",
      "                                                 mae  \n",
      "0  [2025.7380173666934, 2249.2692079872104, 3588....  \n",
      "1  [118.75979411937988, 803.5473245374773, 1690.9...  \n",
      "2  [92.83614852677904, 1026.7711581597152, 1593.7...  \n",
      "3  [93.64895833333382, 551.6394750000018, 1592.38...  \n",
      "4  [253.65396399820506, 942.5478856587439, 3166.0...  \n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'svm_physical'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\\\Performance\\\\PhysicalSystems\\\\svm_physical.csv'\n",
    "path_for_saving_data = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\results_with_normalization_without_PCA\\\\' + dataset_name\n",
    "process_all_svm_physical(dataset_path, dataset_name, path_for_saving_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 13 : svm_simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data):\n",
    "    \n",
    "    ################## Data Preprocessing ######################\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    encoded_data_frame, encoder_isa, encoder_mem_type = encode_text_features('encode', df, \n",
    "                                                                             encoder_isa = None, encoder_mem_type=None)\n",
    "    # total_data = encoded_data_frame.drop(columns = ['arch', 'arch1'])\n",
    "    \n",
    "    total_data = encoded_data_frame.drop(columns = ['arch'])\n",
    "    total_data = total_data.fillna(0)\n",
    "    X_columns = total_data.drop(columns = 'runtime').columns\n",
    "    X = total_data.drop(columns = ['runtime']).to_numpy()\n",
    "    Y = total_data['runtime'].to_numpy()\n",
    "    # X_columns = total_data.drop(columns = 'PS').columns\n",
    "    # X = total_data.drop(columns = ['runtime','PS']).to_numpy()\n",
    "    # Y = total_data['runtime'].to_numpy()\n",
    "    print('Data X and Y shape', X.shape, Y.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print('Train Test Split:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    ################## Data Preprocessing ######################\n",
    "    \n",
    "    # Put best models here using grid search\n",
    "    \n",
    "    # 1. SVR \n",
    "    best_svr = SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "    \n",
    "    # 2. LR\n",
    "    best_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "    \n",
    "    # 3. RR\n",
    "    best_rr = linear_model.Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,\n",
    "      random_state=None, solver='lsqr', tol=0.001)\n",
    "    \n",
    "    # 4. KNN\n",
    "    best_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=15, p=1,\n",
    "                    weights='distance')\n",
    "    \n",
    "    # 5. GPR\n",
    "    best_gpr = GaussianProcessRegressor(alpha=0.01, copy_X_train=True, kernel=None,\n",
    "                         n_restarts_optimizer=0, normalize_y=True,\n",
    "                         optimizer='fmin_l_bfgs_b', random_state=None)\n",
    "    # 6. Decision Tree\n",
    "    best_dt = DecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n",
    "                      max_features='sqrt', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=2, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, presort=False,\n",
    "                      random_state=None, splitter='best')\n",
    "    \n",
    "    # 7. Random Forest \n",
    "    best_rf = RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=3,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                      n_jobs=None, oob_score=False, random_state=None,\n",
    "                      verbose=0, warm_start='True')\n",
    "    \n",
    "    # 8. Extra Trees Regressor\n",
    "    best_etr = ExtraTreesRegressor(bootstrap=False, criterion='friedman_mse', max_depth=3,\n",
    "                    max_features='auto', max_leaf_nodes=None,\n",
    "                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                    min_samples_leaf=1, min_samples_split=2,\n",
    "                    min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "                    oob_score=False, random_state=42, verbose=0,\n",
    "                    warm_start='True')\n",
    "    # 9. GBR\n",
    "    best_gbr = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='mae', init=None,\n",
    "                          learning_rate=0.1, loss='lad', max_depth=3,\n",
    "                          max_features=None, max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=1, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=50,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=42, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "    \n",
    "    # 10. XGB\n",
    "    best_xgb = xgb.XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.5, max_delta_step=0,\n",
    "             max_depth=4, min_child_weight=1, missing=None, n_estimators=10,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, validate_parameters=False, verbosity=1)\n",
    "    \n",
    "    best_models = [best_svr, best_lr, best_rr, best_knn, best_gpr, best_dt, best_rf, best_etr, best_gbr, best_xgb]\n",
    "    best_models_name = ['best_svr', 'best_lr', 'best_rr', 'best_knn', 'best_gpr', 'best_dt', 'best_rf', 'best_etr'\n",
    "                        , 'best_gbr', 'best_xgb']\n",
    "    k = 0\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['model_name', 'dataset_name', 'r2', 'mse', 'mape', 'mae' ])\n",
    "    \n",
    "    for model in best_models:\n",
    "        \n",
    "        print('Running model number:', k+1, 'with Model Name: ', best_models_name[k])\n",
    "        r2_scores = []\n",
    "        mse_scores = []\n",
    "        mape_scores = []\n",
    "        mae_scores = []\n",
    "\n",
    "        # cv = KFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "        cv = ShuffleSplit(n_splits=10, random_state=0)\n",
    "        # print(cv)\n",
    "        \n",
    "        fold = 1\n",
    "        for train_index, test_index in cv.split(X):\n",
    "            model_orig = model\n",
    "            # print(\"Train Index: \", train_index, \"\\n\")\n",
    "            # print(\"Test Index: \", test_index)\n",
    "\n",
    "            X_train_fold, X_test_fold, Y_train_fold, Y_test_fold = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "            # print(X_train_fold.shape, X_test_fold.shape, Y_train_fold.shape, Y_test_fold.shape)\n",
    "            model_orig.fit(X_train_fold, Y_train_fold)\n",
    "            Y_pred_fold = model_orig.predict(X_test_fold)\n",
    "            \n",
    "            # save the folds to disk\n",
    "            data = [X_train_fold, X_test_fold, Y_train_fold, Y_test_fold]\n",
    "            filename = path_for_saving_data + '/folds_data/' + best_models_name[k] +'_'+ str(fold) + '.pickle'\n",
    "            pickle.dump(data, open(filename, 'wb'))\n",
    "            \n",
    "            \n",
    "            # save the model to disk\n",
    "            filename = path_for_saving_data + '/models_data/' + best_models_name[k] + '_' + str(fold) + '.sav'\n",
    "            fold = fold + 1\n",
    "            pickle.dump(model_orig, open(filename, 'wb'))\n",
    "\n",
    "            # some time later...\n",
    "            '''\n",
    "            # load the model from disk\n",
    "            loaded_model = pickle.load(open(filename, 'rb'))\n",
    "            result = loaded_model.score(X_test, Y_test)\n",
    "            print(result)\n",
    "            '''\n",
    "            # scores.append(best_svr.score(X_test, y_test))\n",
    "            '''\n",
    "            plt.figure()\n",
    "            plt.plot(Y_test_fold, 'b')\n",
    "            plt.plot(Y_pred_fold, 'r')\n",
    "            '''\n",
    "            # print('Accuracy =',accuracy_score(Y_test, Y_pred))\n",
    "            r2_scores.append(r2_score(Y_test_fold, Y_pred_fold))\n",
    "            mse_scores.append(mean_squared_error(Y_test_fold, Y_pred_fold))\n",
    "            mape_scores.append(absolute_percentage_error(Y_test_fold, Y_pred_fold))\n",
    "            mae_scores.append(mean_absolute_error(Y_test_fold, Y_pred_fold))\n",
    "        \n",
    "        df = df.append({'model_name': best_models_name[k], 'dataset_name': dataset_name\n",
    "                        , 'r2': r2_scores, 'mse': mse_scores, 'mape': mape_scores, 'mae': mae_scores }, ignore_index=True)\n",
    "        k = k + 1  \n",
    "    print(df.head())\n",
    "    df.to_csv(path_for_saving_data + '.csv')\n",
    "        # print('MSE for 10 folds\\n', mse_scores)\n",
    "        # print('\\nR2 scores for 10 folds\\n', r2_scores)\n",
    "        # print('\\nMAPE for 10 folds\\n', mape_scores)\n",
    "        # print('\\nMAE scores for 10 folds\\n', mae_scores)\n",
    "        # print('\\nMean MSE = ', np.mean(mse_scores), '\\nMedian MSE = ', np.median(mse_scores))\n",
    "        # print('\\nMean R2 score =',np.mean(r2_scores), '\\nMedian R2 scores = ', np.median(r2_scores))\n",
    "        # print('\\nMean Absolute Percentage Error =',np.mean(mape_scores), \n",
    "        #       '\\nMedian Absolute Percentage Error =', np.median(mape_scores))    \n",
    "        # print('\\nMean MAE =',np.mean(mae_scores), \n",
    "        #      '\\nMedian MAE =', np.median(mae_scores)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'dijkstra_physical'\n",
    "dataset_path = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\\\Performance\\\\PhysicalSystems\\\\dijkstra_physical.csv'\n",
    "path_for_saving_data = 'C:\\\\Users\\\\Rajat\\\\Desktop\\\\PROJECT_MODE\\\\Paper_3_Rajat\\\\results_with_normalization_without_PCA\\\\' + dataset_name\n",
    "process_all_dijkstra_physical(dataset_path, dataset_name, path_for_saving_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
